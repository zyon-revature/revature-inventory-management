<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title type="html">Byteman 4.0.17 has been released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rWbhPy3hdCQ/byteman-4017-has-been-released.html" /><author><name>Andrew Dinn</name></author><id>http://bytemanblog.blogspot.com/2021/09/byteman-4017-has-been-released.html</id><updated>2021-09-03T14:47:00Z</updated><content type="html">Byteman 4.0.17 is now available from the and from the . It is the latest update release for use on all JDK9+ runtimes up to and including JDK17.   Byteman 4.0.17 is a maintenance release which provides a few small enhancements and fixes a minor bug. More details are provided in the .  &lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rWbhPy3hdCQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Andrew Dinn</dc:creator><feedburner:origLink>http://bytemanblog.blogspot.com/2021/09/byteman-4017-has-been-released.html</feedburner:origLink></entry><entry><title type="html">Bringing Drools rules into the cloud with Kogito: a step by step path</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GDkJ8DmV0lM/bringing-drools-rules-into-the-cloud-with-kogito-a-step-by-step-path.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2021/09/bringing-drools-rules-into-the-cloud-with-kogito-a-step-by-step-path.html</id><updated>2021-09-02T07:20:36Z</updated><content type="html">The goal of this article is to demonstrate how to expose a Drools stateless rules evaluation in a Quarkus REST endpoint and then how to migrate it to Kogito in order to fully leverage Quarkus features and finally how to embrace the Kogito’s programming model based on rule units. OVERVIEW In Drools a stateless rules evaluation is a one-off execution of a rule set against a provided set of facts. Under this point of view this kind of rules evaluation can be seen as a pure function invocation, where the return value is only determined by its input values, without observable side effects, in which the arguments passed to the function are actually the facts to be inserted into the session and the result is the outcome of your rules set applied on those facts. From a consumer perspective of this service the fact that the invoked function uses a rule engine to perform its job could be only an internal implementation detail. In this situation it is natural to expose such a function through a REST endpoint, thus turning it into a microservice. At this point it can be eventually deployed into a Function as a Service environment, possibly after having compiled it into a native image, to avoid paying the cost of relatively high JVM startup time. This document is focused on this stateless scenario because at the moment it is the only use case also supported in Kogito. THE SAMPLE PROJECT Let’s try to put this idea in practice by taking an existing Drools project and migrating it in steps to Kogito and Quarkus. The domain model of the sample project that we will is use to demonstrate this migration is made only by two classes, a loan application public class LoanApplication {    private String id;    private Applicant applicant;    private int amount;    private int deposit;    private boolean approved = false;    public LoanApplication(String id, Applicant applicant, int amount, int deposit) {        this.id = id;        this.applicant = applicant;        this.amount = amount;        this.deposit = deposit;    } } and the applicant who requested it public class Applicant {    private int age;    public Applicant(String name, int age) {        this.name = name;        this.age = age;    } } The rules set is made of business decisions to approve or reject an application plus one last rule collecting all the approved applications into a list. The rules set is made of business decisions to approve or reject an application plus one last rule collecting all the approved applications into a list. global Integer maxAmount; global java.util.List approvedApplications; rule LargeDepositApprove when    $l: LoanApplication( applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ) then    modify($l) { setApproved(true) }; // loan is approved end rule LargeDepositReject when    $l: LoanApplication( applicant.age &gt;= 20, deposit &gt;= 1000, amount &gt; maxAmount ) then    modify($l) { setApproved(false) }; // loan is rejected end // ... more loans approval/rejections business rules ... rule CollectApprovedApplication when    $l: LoanApplication( approved ) then    approvedApplications.add($l); // collect all approved loan applications end STEP 1: EXPOSING RULES EVALUATION WITH A REST ENDPOINT THROUGH QUARKUS The first goal that we want to achieve is providing a REST endpoint for this service using Quarkus. The easiest way to do this is creating a new module depending on the one containing the rules plus a few basic Quarkus libraries providing the REST support. &lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;    &lt;artifactId&gt;quarkus-resteasy&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;    &lt;artifactId&gt;quarkus-resteasy-jackson&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.example&lt;/groupId&gt;    &lt;artifactId&gt;drools-project&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;/dependency&gt; &lt;dependencies&gt; With this setup it’s easy to create a REST endpoint as it follows. @Path("/find-approved") public class FindApprovedLoansEndpoint {    private static final KieContainer kContainer = KieServices.Factory.get().newKieClasspathContainer();    @POST()    @Produces(MediaType.APPLICATION_JSON)    @Consumes(MediaType.APPLICATION_JSON)    public List&lt;LoanApplication&gt; executeQuery(LoanAppDto loanAppDto) {        KieSession session = kContainer.newKieSession();        List&lt;LoanApplication&gt; approvedApplications = new ArrayList&lt;&gt;();        session.setGlobal("approvedApplications", approvedApplications);        session.setGlobal("maxAmount", loanAppDto.getMaxAmount());        loanAppDto.getLoanApplications().forEach(session::insert);        session.fireAllRules();        session.dispose();        return approvedApplications;    } } Here a KieContainer containing the rules taken from the other module in the classpath is created and put into a static field. In this way it will be possible to reuse the same KieContainer for all subsequent invocations of this endpoint without having to recompile the rules. When the endpoint is invoked it creates a new KieSession from the container, populates it with the objects coming from a DTO resulting from the unmarshalling of the JSON request. public class LoanAppDto {    private int maxAmount;    private List&lt;LoanApplication&gt; loanApplications;    public int getMaxAmount() {        return maxAmount;    }    public void setMaxAmount(int maxAmount) {        this.maxAmount = maxAmount;    }    public List&lt;LoanApplication&gt; getLoanApplications() {        return loanApplications;    }    public void setLoanApplications(List&lt;LoanApplication&gt; loanApplications) {        this.loanApplications = loanApplications;    } } When we call fireAllRules() the session is fired and all our business logic is evaluated against the provided input data. Then the last rule collects all the approved applications into a global list and this list is returned as the result of the computation. After having started Quarkus you can already put this at work invoking the REST endpoint with a JSON request containing the loan applications to be checked and the value for the maxAmount to be used in the rules, like in the the following example curl -X POST -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{"maxAmount":5000,"loanApplications":[ {"id":"ABC10001","amount":2000,"deposit":1000,"applicant":{"age":45,"name":"John"}}, {"id":"ABC10002","amount":5000,"deposit":100,"applicant":{"age":25,"name":"Paul"}}, {"id":"ABC10015","amount":1000,"deposit":100,"applicant":{"age":12,"name":"George"}} ]}' http://localhost:8080/find-approved and you will be returned with a list of the approved applications. [{"id":"ABC10001","applicant":{"name":"John","age":45},"amount":2000,"deposit":1000,"approved":true}] This straightforward approach has the major drawback of not allowing to use some of the most interesting features of Quarkus like the hot reload and the possibility of creating a native image of the project. Those features, as we will see in the next step, are indeed provided by the Kogito extension to Quarkus that in essence makes Quarkus aware of the existence of the drl files,  implementing their hot reload in a similar way to what Quarkus provides out-of-the-box for the java sources. The integration demonstrated up to this point between Drools and Quakus has to be considered no more than an introduction to the next migration step. Since Kogito supports the use of Drools API, and given the advantages it provides in terms of fully functional out-of-the-box Quarkus integration, we strongly suggest to not stop the development of your REST service at this point.  STEP 2: FROM DROOLS TO KOGITO WITHOUT CHANGING (ALMOST) ANYTHING USING THE DROOLS LEGACY API As anticipated, Kogito can provide those missing features, so let’s try to migrate our project to Kogito with the minimal amount of effort. To do this we can use the Quarkus extension for Kogito in conjunction with the kogito-legacy-api allowing us to use the same API of Drools 7. This approach also makes it possible to consolidate the former two modules into a single one. &lt;dependencies&gt;  &lt;dependency&gt;   &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt;   &lt;artifactId&gt;kogito-quarkus-rules&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;   &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt;   &lt;artifactId&gt;kogito-legacy-api&lt;/artifactId&gt;  &lt;/dependency&gt; &lt;/dependencies&gt; In this way no changes at all are required to  the DRL file containing the rules while the former REST endpoint implementation can be rewritten as follows. @Path("/find-approved") public class FindApprovedLoansEndpoint {    @Inject    KieRuntimeBuilder kieRuntimeBuilder;    @POST()    @Produces(MediaType.APPLICATION_JSON)    @Consumes(MediaType.APPLICATION_JSON)    public List&lt;LoanApplication&gt; executeQuery(LoanAppDto loanAppDto) {        KieSession session = kieRuntimeBuilder.newKieSession();        List&lt;LoanApplication&gt; approvedApplications = new ArrayList&lt;&gt;();        session.setGlobal("approvedApplications", approvedApplications);        session.setGlobal("maxAmount", loanAppDto.getMaxAmount());        loanAppDto.getLoanApplications().forEach(session::insert);        session.fireAllRules();        session.dispose();        return approvedApplications;    } } Here the only difference with the former implementation is that the KieSession instead of being created from the KieContainer is created from an automatically injected KieRuntimeBuilder.  The KieRuntimeBuilder is the interface provided by the kogito-legacy-api module that replace the KieContainer and from which it is now possible to create the KieBases and KieSessions exactly as you did with the KieContainer itself. An implementation of the KieRuntimeBuilder interface is automatically generated at compile time by Kogito and injected into the class implementing the REST endpoint. With this change it is possible both to launch quarkus in dev mode thus leveraging its hot reload to make on-the-fly changes also to the rules files that are immediately applied to the running application and to create a native image of your rule based application.   STEP 3: EMBRACING RULE UNITS AND AUTOMATIC REST ENDPOINT GENERATION A rule unit is a new concept introduced in Kogito encapsulating both a set of rules and the facts against which those rules will be matched. It comes with a second abstraction called data source, defining the sources through which the facts are inserted, acting in practice as typed entry-points. There are two types of data sources: * DataStream: an append-only data source * subscribers only receive new (and possibly past) messages * cannot update/remove * stream may also be hot/cold in “reactive streams” terminology * DataStore: data source for modifiable data * subscribers may act upon the data store, by acting upon the fact handle In essence a rule unit is made of 2 strictly related parts: the definition of the fact to be evaluated and the set of rules evaluating them. The first part is implemented with a POJO, that for our loan applications could be something like the following: package org.kie.kogito.queries; import org.kie.kogito.rules.DataSource; import org.kie.kogito.rules.DataStore; import org.kie.kogito.rules.RuleUnitData; public class LoanUnit implements RuleUnitData {    private int maxAmount;    private DataStore&lt;LoanApplication&gt; loanApplications;    public LoanUnit() {        this(DataSource.createStore(), 0);    }    public LoanUnit(DataStore&lt;LoanApplication&gt; loanApplications, int maxAmount) {        this.loanApplications = loanApplications;        this.maxAmount = maxAmount;    }    public DataStore&lt;LoanApplication&gt; getLoanApplications() { return loanApplications; }    public void setLoanApplications(DataStore&lt;LoanApplication&gt; loanApplications) {        this.loanApplications = loanApplications;    }    public int getMaxAmount() { return maxAmount; }    public void setMaxAmount(int maxAmount) { this.maxAmount = maxAmount; } } Here instead of using the LoanAppDto that we introduced to marshall/unmarshall the JSON requests we are binding directly the class representing the rule unit. The two relevant differences are that it implements the org.kie.kogito.rules.RuleUnitData interface and uses a DataStore instead of a plain List to contain the loan applications to be approved. The first is just a marker interface to notify the engine that this class is part of a rule unit definition. The use of a DataStore is necessary to let the rule engine to react to changes of processed fact, this allows the rule engine to react accordingly to the changes by firing new rules and triggering other rules. In the example, the consequences of the rules modify the approved property of the loan applications. Conversely the maxAmount value can be considered a configuration parameter of the rule unit and left as it is: it will automatically be processed during the rules evaluation with the same semantic of a global, and automatically set from the value passed by the JSON request as in the first example, so you will still be allowed to use it in your rules. The second part of the rule unit is the drl file containing the rules belonging to this unit. package org.kie.kogito.queries; unit LoanUnit; // no need to using globals, all variables and facts are stored in the rule unit  rule LargeDepositApprove when    $l: /loanApplications[ applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ] // oopath style then    modify($l) { setApproved(true) }; end rule LargeDepositReject when    $l: /loanApplications[ applicant.age &gt;= 20, deposit &gt;= 1000, amount &gt; maxAmount ] then    modify($l) { setApproved(false) }; end // ... more loans approval/rejections business rules ... // approved loan applications are now retrieved through a query query FindApproved    $l: /loanApplications[ approved ] end This rules file must declare the same package and a unit with the same name of the java class implementing the RuleUnitData interface in order to state that they belong to the same rule unit. This file has also been rewritten using the new OOPath notation: as anticipated, here the data source acts as a typed entry-point and the oopath expression has its name as root while the constraints are in square brackets, like in the following example. $l: /loanApplications[ applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ] Alternatively you can still use the old DRL syntax, specifying the name of the data source as an entry-point, with the drawback that in this case you need to specify again the type of the matched object, even if the engine can infer it from the type of the datasource, as it follows.  $l: LoanApplication( applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ) from entry-point loanApplications Finally note that the last rule collecting all the approved loan applications into a global List has been replaced by a query simply retrieving them. One of the advantages in using a rule unit is that it clearly defines the context of computation, in other terms the facts to be passed in input to the rule evaluation. Similarly the query defines what is the output expected by this evaluation.      This clear definition of the computation boundaries allows Kogito to also automatically generate a class executing the query and returning its results public class LoanUnitQueryFindApproved implements org.kie.kogito.rules.RuleUnitQuery&lt;List&lt;org.kie.kogito.queries.LoanApplication&gt;&gt; {    private final RuleUnitInstance&lt;org.kie.kogito.queries.LoanUnit&gt; instance;    public LoanUnitQueryFindApproved(RuleUnitInstance&lt;org.kie.kogito.queries.LoanUnit&gt; instance) {        this.instance = instance;    }    @Override    public List&lt;org.kie.kogito.queries.LoanApplication&gt; execute() {        return instance.executeQuery("FindApproved").stream().map(this::toResult).collect(toList());    }    private org.kie.kogito.queries.LoanApplication toResult(Map&lt;String, Object&gt; tuple) {        return (org.kie.kogito.queries.LoanApplication) tuple.get("$l");    } } together with a REST endpoint taking the rule unit as input, passing it to the former query executor and returning its as output. @Path("/find-approved") public class LoanUnitQueryFindApprovedEndpoint {    @javax.inject.Inject    RuleUnit&lt;org.kie.kogito.queries.LoanUnit&gt; ruleUnit;    public LoanUnitQueryFindApprovedEndpoint() {    }    public LoanUnitQueryFindApprovedEndpoint(RuleUnit&lt;org.kie.kogito.queries.LoanUnit&gt; ruleUnit) {        this.ruleUnit = ruleUnit;    }    @POST()    @Produces(MediaType.APPLICATION_JSON)    @Consumes(MediaType.APPLICATION_JSON)    public List&lt;org.kie.kogito.queries.LoanApplication&gt; executeQuery(org.kie.kogito.queries.LoanUnit unit) {        RuleUnitInstance&lt;org.kie.kogito.queries.LoanUnit&gt; instance = ruleUnit.createInstance(unit);        return instance.executeQuery(LoanUnitQueryFindApproved.class);    } } You can have as many query as you want and for each of them it will be generated a different REST endpoint with the same name of the query transformed from camel case (like FindApproved) to dash separated (like find-approved). The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GDkJ8DmV0lM" height="1" width="1" alt=""/&gt;</content><dc:creator>Mario Fusco</dc:creator><feedburner:origLink>https://blog.kie.org/2021/09/bringing-drools-rules-into-the-cloud-with-kogito-a-step-by-step-path.html</feedburner:origLink></entry><entry><title>Faster web deployment with Python serverless functions</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Wob7oaCf6hA/faster-web-deployment-python-serverless-functions" /><author><name>Don Schenck</name></author><id>07578a68-80d3-4ada-9dc6-9d404b0f335b</id><updated>2021-09-02T07:00:00Z</updated><published>2021-09-02T07:00:00Z</published><summary type="html">&lt;p&gt;Functions as a Service (FaaS) and &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless architecture&lt;/a&gt; promise quick, lightweight deployments for web applications and other standalone functions. But until recently, creating FaaS in &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; has been a "sort of" process consisting of multiple steps. You weren't really creating a function so much as an application that could scale back to zero pods after a few minutes, then scale up again when called.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;Red Hat OpenShift Serverless Functions&lt;/a&gt; is a newer feature that changes all of that. As a developer, you can use it to deploy functions in a snap. You can scaffold functions that handle HTTP requests or &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; with one command.&lt;/p&gt; &lt;p&gt;This article gets you started with creating and deploying serverless functions with OpenShift Serverless Functions. We'll use &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; to develop our serverless function, but it's just one of many languages you could choose from.&lt;/p&gt; &lt;h2&gt;Creating and deploying serverless functions with Knative&lt;/h2&gt; &lt;p&gt;OpenShift Serverless Functions uses the open source &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Knative&lt;/a&gt; framework, which offers powerful management tools through its &lt;code&gt;kn&lt;/code&gt; command-line interface (CLI). Prior to OpenShift Serverless Functions, creating a function in OpenShift required writing an application from scratch, using Knative to manage the application, and creating the deployment, service, and route to support the application. While creating a serverless function that way was not terribly complicated, OpenShift Serverless Functions makes life much easier.&lt;/p&gt; &lt;p&gt;With OpenShift Serverless Functions, developers no longer have to worry about creating the deployment, service, and route. It's all one thing: The function. You can't get more &lt;em&gt;serverless&lt;/em&gt; than that.&lt;/p&gt; &lt;p&gt;Deploying a function with OpenShift Serverless Functions requires three Knative commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kn func create kn func build kn func deploy &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's more to the process, but those three commands get to the heart of it. We'll explore more about deployment shortly. First, we need to set up our environment to support OpenShift Serverless Functions.&lt;/p&gt; &lt;h2&gt;Step 1: Set up your serverless development environment&lt;/h2&gt; &lt;p&gt;I was able to complete all of my examples for this article using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;. CodeReady Containers requires at least 9GB of RAM. I also had to set the number of CPUs to five in order to get both HTTP-driven and event-driven functions to run at the same time. Note that I issued this command &lt;em&gt;before&lt;/em&gt; starting CodeReady Containers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;crc config set cpus 5&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I used a more enterprise-like, cloud-based OpenShift cluster—as you might find in a typical OpenShift installation—CPU and memory usage was not a concern.&lt;/p&gt; &lt;h3&gt;The OpenShift Serverless Operator&lt;/h3&gt; &lt;p&gt;Before you can start deploying functions to an OpenShift cluster, you must install the OpenShift Serverless Operator. From the OpenShift console, locate the operator's card, click on it, and then use the default values to install it. When the installation is finished, the dashboard will let you know. When you see the "Installed operator — ready for use" message shown in Figure 1, click the &lt;strong&gt;View Operator&lt;/strong&gt; button.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_installed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_installed.png?itok=wQQGBTVK" width="607" height="227" alt="After you successfully install the Red Hat OpenShift Serverless Operator, the OpenShift dashboard shows the message "Installed operator — ready for use."" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The OpenShift dashboard showing the Red Hat OpenShift Serverless Operator is ready for use. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You will see your OpenShift Serverless Operator in all its glory, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_openshift.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_openshift.png?itok=3XPyIrR4" width="977" height="474" alt="The Red Hat OpenShift Serverless Operator offers three APIs: Knative Serving, Knative Eventing, and Knative Kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The OpenShift dashboard showing the APIs offered by the Red Hat OpenShift Serverless Operator. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;The Knative Serving API&lt;/h3&gt; &lt;p&gt;With the operator in place, your next step is to prepare the Knative Serving API. Change the project you're working in to &lt;strong&gt;knative-serving&lt;/strong&gt;, as shown in Figure 3. That's where the API must be located.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_serving.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_serving.png?itok=bOjbeCBo" width="360" height="596" alt="Change the current project to Knative Serving in OpenShift Serverless Functions." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Choosing the knative-serving project in OpenShift Serverless Functions. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once that's done, click on the &lt;strong&gt;Knative Serving&lt;/strong&gt; link under the Provided APIs, click &lt;strong&gt;Create Knative Serving&lt;/strong&gt;, and use the default values to create the API.&lt;/p&gt; &lt;p&gt;When all of the statuses read True, as shown in Figure 4, you are ready to start using OpenShift Serverless Functions for HTTP-based functions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_true.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_true.png?itok=omHA_hE6" width="708" height="280" alt="Knative Serving is ready when all statuses are True." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Status of Knative Serving. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;The Knative Eventing API&lt;/h3&gt; &lt;p&gt;You need to perform the steps in this section if you want to use CloudEvents to fire your functions. In this case, we'll use Knative Eventing with CloudEvents. The steps are similar if you want to use Knative Serving instead.&lt;/p&gt; &lt;p&gt;Change the working project to &lt;strong&gt;knative-eventing&lt;/strong&gt; and make sure the OpenShift Serverless Function Operator page is displayed.&lt;/p&gt; &lt;p&gt;Click on the &lt;strong&gt;Knative Eventing&lt;/strong&gt; link under the Provided APIs, then click &lt;strong&gt;Create Knative Eventing&lt;/strong&gt;. Use the default values to create the API.&lt;/p&gt; &lt;p&gt;When all of the statuses at the bottom of the page read &lt;strong&gt;True&lt;/strong&gt;, you are ready to start using OpenShift Serverless Functions for CloudEvent-based functions.&lt;/p&gt; &lt;p&gt;That's it: We're finished with all of the installation tasks. Our cluster will now support both HTTP-based and CloudEvent-based serverless functions.&lt;/p&gt; &lt;h2&gt;Step 2: Create an HTTP serverless function in Python&lt;/h2&gt; &lt;p&gt;You can create an HTTP serverless function using Knative's &lt;code&gt;kn&lt;/code&gt; CLI, and the function will be fully functional. You do have to edit the code, of course, to do what you want.&lt;/p&gt; &lt;p&gt;The steps required to create a basic function are shown in Figure 5. In a terminal window, create a directory whose name will become the name of the function. Then, move into that directory and create the function using the &lt;code&gt;kn func create&lt;/code&gt; command. The default runtime is &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, which we will not be using. Instead, we'll use the following command to create a serverless function written in Python:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kn func create -l python&lt;/code&gt;&lt;/pre&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_basic.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_basic.png?itok=U01f42x2" width="646" height="299" alt="The steps to create a basic application culminate in a "kn func create" command." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The steps to create a serverless function using Python. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Why did I choose Python? It's popular, I have a Python &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice&lt;/a&gt; that I'm going to convert to a function (in my next article), and Red Hat Developer already has a &lt;a href="https://developers.redhat.com/articles/2021/07/01/nodejs-serverless-functions-red-hat-openshift-part-1-logging"&gt; series of articles about creating OpenShift Serverless Functions with Node.js&lt;/a&gt;. So, Python it is.&lt;/p&gt; &lt;h3&gt;About the kn func create command&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;kn func create&lt;/code&gt; command uses the name of the current directory to create the source code for a function. Any supporting files, such as dependencies, will also be created. You simply start with this template and edit the function to suit your needs.&lt;/p&gt; &lt;p&gt;If no language is specified, Node.js will be the default. Several languages are supported, and the list seems to be growing at a decent pace. For now, you can specify any of the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Go&lt;/li&gt; &lt;li&gt;Node.js&lt;/li&gt; &lt;li&gt;Python&lt;/li&gt; &lt;li&gt;Quarkus&lt;/li&gt; &lt;li&gt;Rust&lt;/li&gt; &lt;li&gt;Spring Boot&lt;/li&gt; &lt;li&gt;TypeScript&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Enter this command to see the list of currently supported languages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kn func create --help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 6 shows where the list of languages appears in the output.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_languages.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_languages.png?itok=jo27UbOi" width="1087" height="709" alt="Output of the "kn func create --help" command shows which languages it supports." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Languages supported by the "kn func create" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Creating the Knative function&lt;/h3&gt; &lt;p&gt;So what just happened in our &lt;code&gt;kn&lt;/code&gt; command? Figure 7 shows a listing in the directory after we run &lt;code&gt;kn func create -l python&lt;/code&gt;.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_create.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_create.png?itok=w8RukE92" width="612" height="240" alt="The "kn func create" command adds files for a basic application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: The content of project directory after running the "kn func create" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Let's look inside the &lt;code&gt;func.py&lt;/code&gt; file to see what was created and how it will be used:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;from parliament import Context def main(context: Context): """ Function template The context parameter contains the Flask request object and any CloudEvent received with the request. """ return { "message": "Howdy!" }, 200&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As shown, this skeleton Python function returns "Howdy!" Remove the comments and you can see that it only takes three lines of code to make a working function. As a default function, the skeleton function is not very useful. My next article will update it to read from a database, so stay tuned.&lt;/p&gt; &lt;p&gt;Note that we've also created the &lt;code&gt;func.yaml&lt;/code&gt; file. If you view the contents, you will notice that it is incomplete. For example, the &lt;code&gt;image&lt;/code&gt; field is empty. If you wish, you can edit this file to create the image name and tag. The default will be the function name and the &lt;code&gt;:latest&lt;/code&gt; tag:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;name: python-faas-example namespace: "" runtime: python image: "" imageDigest: "" builder: quay.io/boson/faas-python-builder:v0.8.3 builderMap: default: quay.io/boson/faas-python-builder:v0.8.3 volumes: [] envs: [] annotations: {} options: {}&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 3: Build the Python serverless function&lt;/h2&gt; &lt;p&gt;We can build our default HTTP-based function by running the &lt;code&gt;kn func build&lt;/code&gt; command. But because the image name was not specified in the &lt;code&gt;func.yaml&lt;/code&gt; file, this command will prompt us for an image registry. It will use the registry name, the function name, and the tag &lt;code&gt;:latest&lt;/code&gt; to create the image name—if you haven't already supplied one by editing the YAML file. For my own functions, I use my registry: &lt;code&gt;docker.io/donschenck&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Figure 8 shows the &lt;code&gt;kn func build&lt;/code&gt; command and the resulting &lt;code&gt;func.yaml&lt;/code&gt; file. Notice that the fully-qualified image name has been generated by the command. I'm using PowerShell in Windows, but a Bash shell terminal in macOS or Linux works just as well. The operating system you choose doesn't affect the results: You can build functions anywhere.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_build.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_build.png?itok=UiWYu5dl" width="721" height="353" alt="The "kn func build" command creates a minimal YAML configuration file." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: YAML configuration created by the "kn func build" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If you view your local image registry, shown in Figure 9, you will see that the image now exists. (I have no idea why "41 years ago" appears.)&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_images.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_images.png?itok=L-OTemY7" width="1255" height="102" alt="A "docker images" command shows the image you created." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: A "docker images" command showing the existence of an image. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Testing the function&lt;/h3&gt; &lt;p&gt;You can use the &lt;code&gt;kn func run&lt;/code&gt; command to run the function locally and test it. In this case, the function will run on port 8080.&lt;/p&gt; &lt;h2&gt;Step 4: Deploy the Python serverless function&lt;/h2&gt; &lt;p&gt;With the function built into an image on your local machine, it's time to deploy it to a cluster. Before you can do that, you need to sign into two systems: The image registry you're using (mine is &lt;code&gt;docker.io/donschenck&lt;/code&gt;) and the cluster where you wish to deploy the function. You also need to make sure you're in the correct project. Figure 10 shows an example of what to do.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_logins.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_logins.png?itok=yy7mz71E" width="1179" height="506" alt="To prepare for deployment to OpenShift, you must log in to the image registry and your OpenShift cluster, and create a new project." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: Summary of logins and creation of a project in OpenShift. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;When you run &lt;code&gt;kn func deploy&lt;/code&gt;, the command builds the image, pushes the image to the image registry you specified, and then deploys that image from the registry into the OpenShift project to which your current context is set.&lt;/p&gt; &lt;p&gt;In this case, the &lt;code&gt;docker.io/donschenck/python-faas-example:latest&lt;/code&gt; image is deployed to the &lt;code&gt;faas-example&lt;/code&gt; project in my cluster, as shown in Figure 11.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_deploy.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_deploy.png?itok=wVpOkk3z" width="1179" height="84" alt="The "kn func deploy" command can get an application into your cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: Output from the "kn func deploy" command in a cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can edit the &lt;code&gt;func.yaml&lt;/code&gt; file and change the image tag if you wish. I changed my tag from &lt;code&gt;:latest&lt;/code&gt; to &lt;code&gt;:v1&lt;/code&gt; and it works just fine.&lt;/p&gt; &lt;p&gt;Figure 12 shows the developer topology view in the OpenShift dashboard, displaying the deployed function.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_dashboard.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_dashboard.png?itok=WfB0MVxa" width="504" height="404" alt="The OpenShift dashboard shows that your function is deployed and has running instances." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: The OpenShift dashboard showing the deployed function. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can prove that the function in the cluster is working simply by clicking on the &lt;strong&gt;Open URL&lt;/strong&gt; icon.&lt;/p&gt; &lt;h3&gt;Watch the HTTP function scale to zero&lt;/h3&gt; &lt;p&gt;Wait a bit and you'll see the dark blue circle in the function turn white (see Figure 13). This means the function is still available, but it has scaled down to zero pods.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_scaled.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_scaled.png?itok=IjKvh6A5" width="509" height="400" alt="When the blue circle around a function disappears in the OpenShift dashboard, the function has scaled down to zero pods." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: The function after it has scaled down to zero pods. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you access the function now—by clicking on the &lt;strong&gt;Open URL&lt;/strong&gt; icon, or refreshing the browser where you previously opened it—you'll see a slight delay before getting the result. This delay happens only when the function is scaling from zero to one. Refresh yet again and you'll see a speedy response. The function is now up and running.&lt;/p&gt; &lt;h3&gt;Update the function&lt;/h3&gt; &lt;p&gt;Updating the function requires the following steps, which are shown in Figure 14:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Edit the &lt;code&gt;func.py&lt;/code&gt; source file.&lt;/li&gt; &lt;li&gt;Run the &lt;code&gt;kn func deploy&lt;/code&gt; command.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;That's all you need to do. The &lt;code&gt;kn func deploy&lt;/code&gt; command &lt;em&gt;automagically&lt;/em&gt; rebuilds the image before pushing it to your image registry and deploying it to your cluster.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_updated.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_updated.png?itok=9guPcOWj" width="1173" height="296" alt="Updating a function requires editing the source code and redeploying it with the "kn func deploy" command." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 14: Steps needed to update a function. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Troubleshooting kn func deploy&lt;/h2&gt; &lt;p&gt;Before closing, let's look at some common error messages related to &lt;code&gt;kn func deploy&lt;/code&gt; and how to recover from them.&lt;/p&gt; &lt;h3&gt;incorrect username or password&lt;/h3&gt; &lt;p&gt;This message, shown in Figure 15, occurred to me once when I ran &lt;code&gt;kn func deploy&lt;/code&gt; while I was not logged into my docker.io registry.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_incorrect.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_incorrect.png?itok=mKtPZy5O" width="476" height="25" alt="An invalid password or username when logging in prevents deployment of the functon to the image registry." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 15: An "incorrect username or password" error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The lesson is that you must be logged into the image register in order to successfully run the command, because it has to push the image to the repository. The &lt;code&gt;kn&lt;/code&gt; command was nice enough to prompt me for username and password, but I made a mistake entering them. Of course, my function was not deployed as a result. When I supplied the correct name and password, the command worked.&lt;/p&gt; &lt;h3&gt;knative deployer failed to get the Knative Service&lt;/h3&gt; &lt;p&gt;This happened to me when I ran &lt;code&gt;kn func deploy&lt;/code&gt; while I was not logged into my OpenShift cluster, as shown in Figure 16. The deployment failed.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_failed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_failed.png?itok=ngLoq_Tf" width="1016" height="64" alt="If you are not logged in to your project in your OpenShift cluster, the "kn func deploy" command cannot get access to the cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 16: A "knative deployer failed to get the Knative Service" error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Although the &lt;code&gt;kn&lt;/code&gt; command can gracefully log in to the image repository, as shown in the previous section, it cannot automatically connect to a cluster. Make sure to log in to the cluster and the correct project, then rerun the &lt;code&gt;kn&lt;/code&gt; command.&lt;/p&gt; &lt;h3&gt;timeout&lt;/h3&gt; &lt;p&gt;I got this error when I ran &lt;code&gt;kn func deploy&lt;/code&gt; while using Red Hat's quay.io as my image registry, as shown in Figure 17.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_timeout.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/ofs_timeout.png?itok=VjsK8Byi" width="600" height="12" alt="A timeout error may appear when you don't explicitly make images Public in Red Hat's quay.io registry." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 17: A "timeout" error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;By default, images added to quay.io registry are marked Private, so your OpenShift cluster cannot pull the image. Simply change the repository visibility in quay.io to Public. OpenShift will continue to attempt to pull the image, and once it is publicly available, the deployment will succeed.&lt;/p&gt; &lt;h2&gt;What else can I do with Python serverless functions?&lt;/h2&gt; &lt;p&gt;Look for the next article in this series, where we'll build a Python-based serverless function that responds to a CloudEvent instead of an HTTP request. Also visit the &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;OpenShift Serverless&lt;/a&gt; homepage to learn more about creating, scaling, and managing serverless functions on &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Serverless functions in Java and Node.js&lt;/h2&gt; &lt;p&gt;Are you interested in writing serverless functions in &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;? Start with &lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;this overview of OpenShift serverless functions&lt;/a&gt;, then get a quick tutorial introduction to &lt;a href="https://developers.redhat.com/blog/2021/01/29/write-a-quarkus-function-in-two-steps-on-red-hat-openshift-serverless"&gt;writing a Quarkus function in two steps&lt;/a&gt; or &lt;a href="https://developers.redhat.com/articles/2021/07/01/nodejs-serverless-functions-red-hat-openshift-part-1-logging"&gt;developing Node.js serverless functions&lt;/a&gt; on Red Hat OpenShift.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/02/faster-web-deployment-python-serverless-functions" title="Faster web deployment with Python serverless functions"&gt;Faster web deployment with Python serverless functions&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Wob7oaCf6hA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-09-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/02/faster-web-deployment-python-serverless-functions</feedburner:origLink></entry><entry><title>The outbox pattern with Apache Kafka and Debezium</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VFcQCM_rYaU/outbox-pattern-apache-kafka-and-debezium" /><author><name>Don Schenck</name></author><id>fde9b237-3f15-4ece-a630-b4f9c5c97aee</id><updated>2021-09-01T12:00:00Z</updated><published>2021-09-01T12:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices"&gt;Microservices&lt;/a&gt; need access to shared data. Microservices also need to be loosely coupled. Just how are developers supposed to reconcile these diametrically opposed ideas?&lt;/p&gt; &lt;p&gt;Enter &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt;, &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt;, and &lt;a href="https://microservices.io/patterns/data/transactional-outbox.html"&gt;the outbox pattern&lt;/a&gt;. By combining messaging and change data capture technologies with good programming practices, you can meet both microservices demands with ease.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wEhr-mnPOeQ"&gt;This video explores the outbox pattern and demonstrates it in action&lt;/a&gt;, using &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt;. In just minutes, you’ll see how easy it is to replicate data to your microservices while keeping them loosely coupled. OpenShift Streams for Apache Kafka is a managed cloud solution with a free-to-try introductory option.&lt;/p&gt; &lt;p&gt;When you’re finished, you can use a free account on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; to start your own education and experimentation.&lt;/p&gt; &lt;p&gt;You can explore &lt;a href="https://developers.redhat.com/articles/2021/08/11/how-maximize-data-storage-microservices-and-kubernetes-part-1-introduction"&gt;other data solutions within Red Hat OpenShift&lt;/a&gt; as well. The choices are many, so begin your journey today.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/01/outbox-pattern-apache-kafka-and-debezium" title="The outbox pattern with Apache Kafka and Debezium"&gt;The outbox pattern with Apache Kafka and Debezium&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VFcQCM_rYaU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-09-01T12:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/01/outbox-pattern-apache-kafka-and-debezium</feedburner:origLink></entry><entry><title>Red Hat CodeReady Containers 1.31.2 makes the leap</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/50_u54cQr0w/red-hat-codeready-containers-1312-makes-leap" /><author><name>CodeReady Containers Team</name></author><id>a24b60ae-7ffc-470d-8a00-111abeca5c91</id><updated>2021-09-01T07:00:00Z</updated><published>2021-09-01T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/09/05/red-hat-openshift-4-on-your-laptop-introducing-red-hat-codeready-containers"&gt;Red Hat CodeReady Containers&lt;/a&gt; supports local development and testing on a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster. We recently released CodeReady Containers 1.31.2, which is the first version based on the major &lt;a href="https://cloud.redhat.com/blog/red-hat-openshift-4.8-is-now-generally-available"&gt;OpenShift 4.8&lt;/a&gt; release. The CodeReady Containers team doesn't publicly report our advances on a regular basis, so this article is a good opportunity to learn about the biggest changes to CodeReady Containers during the past several months.&lt;/p&gt; &lt;h2&gt;Upgrade to OpenShift 4.8&lt;/h2&gt; &lt;p&gt;With this release, we updated CodeReady Containers to use the 4.8 release of OpenShift. This release offers support for single-node clusters as a developer preview. We also enabled OpenShift’s Machine Config Operator, so users can now follow OpenShift documentation for registry and proxy configuration, or for any other changes that use the Machine Config Operator to modify the cluster. In previous releases, these changes required steps specific to CodeReady Containers. The following video shows how to enable the Machine Config Operator (MCO).&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Installers&lt;/h2&gt; &lt;p&gt;On macOS and Windows 10, we are now shipping native installers (&lt;code&gt;.pkg&lt;/code&gt; and &lt;code&gt;.msi&lt;/code&gt; files, respectively). The native installers provide an easier installation procedure, with more integrated requirement checks. The following video shows how installation on macOS can be done in less than 30 seconds.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt;The installers are signed so that they can be properly validated by the operating system before installation, which is particularly important in light of recent supply-chain attacks. Installers are the only supported way to install and use CodeReady Containers on macOS and Windows.&lt;/p&gt; &lt;h2&gt;System trays&lt;/h2&gt; &lt;p&gt;The new installers on macOS and Windows come with a system tray icon. This icon allows direct interactions with CodeReady Containers, such as to start and stop your cluster. You no longer need to fall back to a shell prompt to manage your OpenShift instance.&lt;/p&gt; &lt;h2&gt;New networking stack&lt;/h2&gt; &lt;p&gt;One of the biggest pain points for CodeReady Containers over the years has been networking, particularly in corporate environments. Configuring the host system DNS to redirect the &lt;code&gt;crc.testing&lt;/code&gt; domain to the CodeReady Containers virtual machine often required superuser privileges. Additionally, corporate VPNs or firewalls would sometimes get in the way and prevent this setup from working, resulting in cluster connectivity issues for end users.&lt;/p&gt; &lt;p&gt;A few releases ago, we started moving to a userland networking stack based on &lt;a href="https://github.com/containers/gvisor-tap-vsock"&gt;gvisor&lt;/a&gt;. All networking communication by the virtual machine now goes through a CodeReady Containers daemon running on the host. Together with improved usage of the &lt;code&gt;/etc/hosts&lt;/code&gt; file for DNS resolution, this change makes the networking setup less reliant on modifications to the host configuration. It also avoids some of the aforementioned issues with corporate networks. This new networking stack is now the default on Windows and macOS.&lt;/p&gt; &lt;h2&gt;Disk expansion&lt;/h2&gt; &lt;p&gt;CodeReady Containers instances use a 31GB disk image by default. This is not enough for some users who want to deploy heavy workloads.&lt;/p&gt; &lt;p&gt;It’s now possible, when running &lt;code&gt;crc start&lt;/code&gt;, to use the &lt;code&gt;--disk-size&lt;/code&gt; or &lt;code&gt;-d&lt;/code&gt; command-line option to dynamically resize the disk to the desired size. The disk size can only be expanded, not reduced. The following example uses a 40GB disk for the CodeReady Containers instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ crc start --disk-size 40&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On macOS and Windows, you can also use the system tray icon to easily change the disk size.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has summarized the most notable changes the CodeReady Containers team made during the past few months, but there were also plenty more minor improvements and bug fixes. We strongly encourage you to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_containers/1.31"&gt;try CodeReady Containers 1.31.2&lt;/a&gt; and report any problems you find.&lt;/p&gt; &lt;p&gt;We’ll keep polishing and improving CodeReady Containers in the months to come. Our roadmap includes Podman integration and improved integration with remote CodeReady Containers instances, so stay tuned!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/01/red-hat-codeready-containers-1312-makes-leap" title="Red Hat CodeReady Containers 1.31.2 makes the leap"&gt;Red Hat CodeReady Containers 1.31.2 makes the leap&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/50_u54cQr0w" height="1" width="1" alt=""/&gt;</summary><dc:creator>CodeReady Containers Team</dc:creator><dc:date>2021-09-01T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/01/red-hat-codeready-containers-1312-makes-leap</feedburner:origLink></entry><entry><title type="html">An Extension for Long Running Activities</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w-jurrmJCHY/" /><author><name>Michael Musgrove</name></author><id>https://quarkus.io/blog/using-lra/</id><updated>2021-09-01T00:00:00Z</updated><content type="html">Introduction The Quarkus LRA extension is useful for building JAX-RS services that wish to definitively agree when an interaction has finished, with either a successful outcome or an unsuccessful one. In the successful case, all participants can clean up in the knowledge that all other services will do so as...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w-jurrmJCHY" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://quarkus.io/blog/using-lra/</feedburner:origLink></entry><entry><title>Why should I choose Quarkus over Spring for my microservices?</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ma5T-Pjb6MM/why-should-i-choose-quarkus-over-spring-my-microservices" /><author><name>Eric Deandrea</name></author><id>f6484334-5e02-4fbb-bdd4-365927273e1c</id><updated>2021-08-31T14:00:00Z</updated><published>2021-08-31T14:00:00Z</published><summary type="html">&lt;p&gt;As interest grows in &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; developers have struggled to make applications smaller and faster to meet today’s demands and requirements. In the modern computing environment, applications must respond to requests quickly and efficiently, be suitable for running in volatile environments such as virtual machines or containers, and support rapid development. Because of this, Java, and popular Java runtimes, are sometimes considered inferior to runtimes in other languages such as &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; and &lt;a href="https://golang.org/"&gt;Go&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Java language and platform have been very successful over the years, preserving Java as the predominant language in current use. &lt;a href="https://www.grandviewresearch.com/industry-analysis/application-server-market"&gt;Analysts have estimated the global application server market size&lt;/a&gt; at $15.84 billion in 2020, with expectations of growing at a rate of 13.2% from 2021 to 2028. Additionally, tens of millions of Java developers worldwide work for organizations that run their businesses using Java. Faced with today’s challenges, these organizations need to adapt and adopt new ways of building and deploying applications. Forgoing Java for other application stacks isn’t a choice for many organizations. It would involve re-training their development staff and re-implementing processes to release and monitor applications in production.&lt;/p&gt; &lt;h2&gt;Java is still relevant&lt;/h2&gt; &lt;p&gt;With additions to Java and Java frameworks over the past few years, Java can proudly retain its role as the primary language for enterprise applications. &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;, an open source project introduced by Red Hat, is one such framework that has taken the Java community by storm. Quarkus combines developer productivity and joy with the speed and performance of Go.&lt;/p&gt; &lt;p&gt;Quarkus integrates with other modern Java frameworks and libraries that many developers are already familiar with and most likely using. Among the specifications and technologies underlying and integrated with Quarkus are Eclipse MicroProfile, Eclipse Vert.x, Contexts and Dependency Injection (CDI), Jakarta RESTful Web Services (JAX-RS), the Java Persistence API (JPA), the Java Transaction API (JTA), Apache Camel, and Hibernate, just to name a few.&lt;/p&gt; &lt;p&gt;Quarkus and &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring&lt;/a&gt; address many of the same types of applications, but because it was born in today’s day and age, Quarkus has the advantage of starting with a clean slate. Quarkus can focus on innovation in modern areas of development pertaining to scalable, cloud-hosted applications because it doesn’t have to retrofit new patterns and principles into an existing codebase that has evolved over time.&lt;/p&gt; &lt;h2&gt;But I already know Spring ...&lt;/h2&gt; &lt;p&gt;&lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; introduces Quarkus to Java developers with a special eye to helping those familiar with Spring’s concepts, constructs, and conventions learn Quarkus quickly. Spring developers should immediately recognize and be able to apply patterns they are already familiar with, and in many instances, using the same underlying technologies. Using Kotlin in your Spring applications? Great—you can continue using Kotlin with Quarkus.&lt;/p&gt; &lt;p&gt;Chapters are devoted to getting started, RESTful applications, persistence, &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt; services, and cloud environments such as containers and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Each chapter offers like-for-like examples and emphasizes testing patterns and practices with such applications, while also differentiating Quarkus from Spring.&lt;/p&gt; &lt;p&gt;Additionally, Quarkus provides a set of &lt;a href="https://quarkus.io/guides/spring-di#more-spring-guides"&gt;extensions for various Spring APIs&lt;/a&gt;. These extensions help simplify the process of learning Quarkus or migrating existing Spring applications to Quarkus, capitalizing on a developer’s Spring knowledge to accelerate the learning curve to adopt Quarkus. In some cases, an &lt;a href="https://developers.redhat.com/blog/2021/02/09/spring-boot-on-quarkus-magic-or-madness"&gt;existing Spring application may even be able to run in Quarkus&lt;/a&gt; without any code changes.&lt;/p&gt; &lt;h2&gt;How can Quarkus help me?&lt;/h2&gt; &lt;p&gt;Quarkus has many features and capabilities that can help both developers and operations teams.&lt;/p&gt; &lt;h3&gt;Enhancing developer productivity&lt;/h3&gt; &lt;p&gt;Since its inception in early 2019, Quarkus has focused on more than just delivering features. Developer productivity and joy have been critical goals. With every new feature, Quarkus carefully considers the developer experience and how to improve it.&lt;/p&gt; &lt;p&gt;The development process is faster and more pleasant with Quarkus's live coding feature. Quarkus can automatically detect changes made to Java and other resource and configuration files, then transparently re-compile and re-deploy the changes. Usually, within a second, you can view your application’s output or compiler error messages. This feature can also be used with Quarkus applications running in a remote environment. The remote capability is useful where rapid development or prototyping is needed but provisioning services in a local environment isn’t feasible or possible.&lt;/p&gt; &lt;p&gt;Quarkus takes this concept a step further with its &lt;a href="https://youtu.be/0JiE-bRt-GU"&gt;continuous testing&lt;/a&gt; feature to facilitate test-driven development. As changes are made to the application source code, Quarkus can automatically rerun affected tests in the background, giving developers instant feedback about the code they are writing or modifying.&lt;/p&gt; &lt;p&gt;Need a database for your application? Kafka broker? Redis server? AMQP broker? OpenID Connect authentication server? API/Schema registry? Quarkus &lt;a href="https://quarkus.io/guides/datasource#dev-services-configuration-free-databases"&gt;Dev Services for databases&lt;/a&gt; (see the &lt;a href="https://youtu.be/szza3DZlKzA"&gt;video demo&lt;/a&gt;), &lt;a href="https://quarkus.io/guides/kafka-dev-services"&gt;Dev Services for Kafka&lt;/a&gt; (see the &lt;a href="https://youtu.be/z2ZceqVQ20E"&gt;video demo&lt;/a&gt;), &lt;a href="https://quarkus.io/guides/redis-dev-services"&gt;Dev Services for Redis&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/amqp-dev-services"&gt;Dev Services for AMQP&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/security-openid-connect-dev-services"&gt;Dev Services for OpenID Connect&lt;/a&gt; (see the &lt;a href="https://youtu.be/coG5ZbLgjJs"&gt;video demo&lt;/a&gt;), and &lt;a href="https://quarkus.io/guides/apicurio-registry-dev-services"&gt;Dev Services for Apicurio Registry&lt;/a&gt; have you covered. Dev Services makes development faster by providing needed infrastructure automatically, eliminating all the required provisioning and configuration hassle. New Dev Services are added with each new release.&lt;/p&gt; &lt;p&gt;Following the philosophy of simplicity and enhancing developer productivity, &lt;a href="https://quarkus.io/guides/building-native-image"&gt;building an application into a native image&lt;/a&gt; is extremely simple. All the heavy-lifting and integration to consume GraalVM is done for you by the Quarkus build tools. Developers or &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; systems simply need to run a build, just like any other Java build, to produce a native executable. Tests can even be run against the built artifact.&lt;/p&gt; &lt;h3&gt;Kubernetes native&lt;/h3&gt; &lt;p&gt;From the beginning, Quarkus was designed around Kubernetes-native philosophies, optimizing for low memory usage and fast startup times. As much processing as possible is done at build time. Classes used only at application startup are invoked at build time and not loaded into the runtime JVM, reducing the size, and ultimately the memory footprint, of the application running on the JVM.&lt;/p&gt; &lt;p&gt;This design accounted for native compilation from the onset, enabling Quarkus to be "natively native." Similar native capabilities in Spring are still considered experimental or beta, and in some instances, not even available. Coupled with a runtime platform like Kubernetes, more Quarkus applications can be deployed within a given set of resources than other Java or Spring applications.&lt;/p&gt; &lt;h3&gt;To be or not to be reactive?&lt;/h3&gt; &lt;p&gt;With Spring, a developer needs to decide up front, before writing a line of code, which architecture to follow for an application. This choice determines the entire set of libraries that a developer uses in a Spring application. Quarkus does not have such limitations because it was born in the reactive era. Quarkus, at its core, is based on a fully reactive and non-blocking architecture powered by &lt;a href="https://vertx.io"&gt;Eclipse Vert.x&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Quarkus integrates deeply with Vert.x, allowing developers to utilize both blocking (imperative) and non-blocking (reactive) libraries and APIs. In most cases, developers can use both blocking and reactive APIs within the same classes. &lt;a href="https://quarkus.io/blog/resteasy-reactive-smart-dispatch/"&gt;Quarkus ensures&lt;/a&gt; that the blocking APIs will block appropriately while the reactive APIs remain non-blocking.&lt;/p&gt; &lt;h2&gt;Ready to give it a try?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://red.ht/dev-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which offers a free and ready-made environment for trying out containerized applications, is &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;set up to support Quarkus&lt;/a&gt;. Whether you run on-premises, on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat OpenShift&lt;/a&gt;, or in another cloud setting, the many open source capabilities of Quarkus are available.&lt;/p&gt; &lt;p&gt;&lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; will help get you started and guide you through your journey.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices" title="Why should I choose Quarkus over Spring for my microservices?"&gt;Why should I choose Quarkus over Spring for my microservices?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ma5T-Pjb6MM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2021-08-31T14:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices</feedburner:origLink></entry><entry><title>Game telemetry with Kafka Streams and Quarkus, Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qjWAG2-BvTg/game-telemetry-kafka-streams-and-quarkus-part-2" /><author><name>Evan Shortiss</name></author><id>755954fe-2569-44ba-949c-fdd7393a6cc4</id><updated>2021-08-31T07:00:00Z</updated><published>2021-08-31T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2021/08/24/game-telemetry-kafka-streams-and-quarkus-part-1"&gt;first half of this article&lt;/a&gt; introduced &lt;a href="https://arcade.redhat.com/shipwars"&gt;Shipwars&lt;/a&gt;, a browser-based video game that’s similar to the classic &lt;a href="https://en.wikipedia.org/wiki/Battleship_(game)"&gt;Battleship&lt;/a&gt; tabletop game, but with a server-side AI opponent. We set up a development environment to analyze real-time gaming data and I explained some of the ways you might use game data analysis and telemetry data to improve a product.&lt;/p&gt; &lt;p&gt;In this second half, we'll run the analytics and use the captured data to replay games.&lt;/p&gt; &lt;h2&gt;Using Kafka Streams for game analytics&lt;/h2&gt; &lt;p&gt;We're using the &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams API&lt;/a&gt; along with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. Please see the first half of this article to set up the environment for analyzing data captured during gameplay.&lt;/p&gt; &lt;p&gt;The Shipwars architecture contains several independent Java applications that use the Kafka Streams API. The source code for these applications is in the &lt;a href="https://github.com/evanshortiss/shipwars-streams"&gt;shipwars-streams repository&lt;/a&gt; on GitHub. We'll deploy each of the applications for game data analysis.&lt;/p&gt; &lt;h2&gt;Deploy the Kafka Streams enricher&lt;/h2&gt; &lt;p&gt;The first Kafka Streams application you’ll deploy is an &lt;em&gt;enricher&lt;/em&gt;. It performs a join between two sources of data: Players and Attacks (also known as "shots"). Specifically, this application performs a &lt;a href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#kstream-globalktable-join"&gt;KStream-GlobalKTable join&lt;/a&gt;. The join involves deserializing the JSON data from two Kafka topics into POJOs using &lt;a href="https://kafka.apache.org/10/documentation/streams/developer-guide/datatypes"&gt;Serdes&lt;/a&gt; for serialization and deserialization. Then, we use the &lt;a href="https://kafka.apache.org/documentation/streams/developer-guide/"&gt;Kafka Streams DSL&lt;/a&gt; to join the data sources.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;GlobalKTable&lt;/code&gt; stores key-value pairs. In this application, the key is the player ID and the value is the player data; that is, the username and whether they are a (supposed) human or an AI bot. An entry is added to this table each time a player is created by the game server, because the game server emits an event with the player data to the &lt;code&gt;shipwars-players&lt;/code&gt; topic that the &lt;code&gt;GlobalKTable&lt;/code&gt; is subscribed to.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;KStream&lt;/code&gt; is an abstraction above a stream of events in a specific topic. Kafka Streams applications can map, filter, and even join &lt;code&gt;KStream&lt;/code&gt; instances with one another and instances of &lt;code&gt;GlobalKTable&lt;/code&gt;. The Shipwars application demonstrates this feature.&lt;/p&gt; &lt;p&gt;Every attack made in Shipwars arrives on a &lt;code&gt;KStream&lt;/code&gt; subscribed to the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic. It is then joined with the associated player data in the &lt;code&gt;GlobalKTable&lt;/code&gt; to create a new record. The relevant code is included in Figure 1. Also see the &lt;a href="https://github.com/evanshortiss/shipwars-streams/blob/main/shot-stream-enricher/src/main/java/org/acme/kafka/streams/enricher/streams/TopologyShotMapper.java#L29-L82"&gt;a TopologyShotMapper repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_1.png?itok=bkI7ybai" width="600" height="688" alt="The enricher is a Kafka Streams application that performs a KStream-GlobalKTable join and writes the join result to a new Kafka topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Kafka Streams DSL code for the enricher application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Deploy the application&lt;/h3&gt; &lt;p&gt;Take the following steps to deploy the Kafka Streams enricher application:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate to your project in the Developer Sandbox UI.&lt;/li&gt; &lt;li&gt;Ensure that the &lt;strong&gt;Developer&lt;/strong&gt; view is selected in the side menu.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;+Add&lt;/strong&gt; link and select &lt;strong&gt;Container Image&lt;/strong&gt; from the available options.&lt;/li&gt; &lt;li&gt;Paste &lt;code&gt;quay.io/evanshortiss/shipwars-streams-shot-enricher&lt;/code&gt; into the &lt;strong&gt;Image name field&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select Quarkus as the &lt;strong&gt;Runtime icon&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;General&lt;/strong&gt;, select &lt;strong&gt;Create Application&lt;/strong&gt; and enter &lt;code&gt;shipwars-analysis&lt;/code&gt; as the &lt;strong&gt;Application name&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Leave the &lt;strong&gt;Name&lt;/strong&gt; as the default value.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Deployment&lt;/strong&gt; link under the &lt;strong&gt;Advanced options&lt;/strong&gt;. This reveals a new form element to enter environment variables.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Add from ConfigMap or Secret&lt;/strong&gt; button and create the following variables (also shown in Figure 2): &lt;ol&gt;&lt;li&gt;&lt;code&gt;KAFKA_BOOTSTRAP_SERVERS&lt;/code&gt;: Select the generated &lt;code&gt;shipwars&lt;/code&gt; secret and use the &lt;code&gt;bootstrapServers&lt;/code&gt; key as the value.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KAFKA_CLIENT_ID&lt;/code&gt;: Select the &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt; secret and use the &lt;code&gt;client-id&lt;/code&gt; value.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KAFKA_CLIENT_SECRET&lt;/code&gt;: Select the &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt; secret and use the &lt;code&gt;client-secret&lt;/code&gt; value.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button to deploy the application.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_2.jpg?itok=dSf893Vt" width="600" height="366" alt="The Deployment screen in the OpenShift console can configure the Kafka Streams enricher application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Configuring the Kafka Streams enricher application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Play a round of Shipwars&lt;/h3&gt; &lt;p&gt;Now you’ve successfully deployed the first Kafka Streams application. It should appear as a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; icon in the OpenShift Topology view. The blue ring around it indicates that the application is in a healthy running state.&lt;/p&gt; &lt;p&gt;Next, open the &lt;code&gt;shipwars-client&lt;/code&gt; NGINX application URL, then open the Kafka Streams &lt;code&gt;shipwars-streams-shot-enricher&lt;/code&gt; application pod logs in another browser window. Play a match of Shipwars and watch the pod logs as you play. The &lt;code&gt;shipwars-streams-shot-enricher&lt;/code&gt; application prints a log each time it receives an attack event, and joins the event information with the associated player record. This joined record is then written to the &lt;code&gt;shipwars-attacks-lite&lt;/code&gt; Kafka topic.&lt;/p&gt; &lt;p&gt;If you’d like to view the data in the new topic, you can do so using a Kafka client such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt;, which is shown in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_3.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_3.jpg?itok=_CETulbe" width="600" height="366" alt="The kafkacat CLI can show enriched shots. All the shots displayed in this screenshot are from the same match between an AI player and a human player. " typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Using the kafkacat CLI to view enriched shots between an AI and a human player. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Deploy the Kafka Streams aggregators&lt;/h2&gt; &lt;p&gt;The next two Kafka Streams applications perform aggregations. These aggregations are stateful operations that track the rolling state of a value for a particular key.&lt;/p&gt; &lt;h3&gt;Shot distribution&lt;/h3&gt; &lt;p&gt;The first application, the shot distribution aggregator, aggregates the total number of shots against each cell on the 5x5 grid used by Shipwars. This aggregate breaks down into the following integer values:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;AI hit&lt;/li&gt; &lt;li&gt;AI miss&lt;/li&gt; &lt;li&gt;Human hit&lt;/li&gt; &lt;li&gt;Human miss&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To create this aggregation, you need the data output by the Kafka Streams enricher you deployed previously. The data in the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic doesn’t specify whether the attacker was a human or AI. The joined data does specify this important piece of information.&lt;/p&gt; &lt;p&gt;There are two other interesting aspects of this application:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;It uses &lt;a href="https://github.com/eclipse/microprofile-reactive-streams-operators]"&gt;MicroProfile Reactive Streams &lt;/a&gt;to expose the enriched data stream via HTTP server-sent events. You’ll see this in action soon.&lt;/li&gt; &lt;li&gt;It stores the aggregated data and exposes it for queries via HTTP REST endpoints.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;You will find the Kafka Streams DSL code for the shot distribution aggregator in the &lt;a href="https://github.com/evanshortiss/shipwars-streams/blob/main/shot-distribution-aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/TopologyShotAnalysis.java"&gt;TopologyShotAnalysis repository&lt;/a&gt;. Figure 4 shows the relevant code.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_4.png?itok=plS7ahz2" width="600" height="539" alt="Aggregation using the Kafka Streams DSL. This code consumes the enriched attacks or shots, and creates an aggregation that contains counts of hits and misses. The aggregation is keyed for each deployment of the game server. New deployments of the game server result in new aggregate records." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Aggregation using the Kafka Streams DSL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We used the second aggregation at Red Hat Summit 2021. It forms a complete record of all turns for each match, so that you can analyze them later or even replay them as we did in our demonstration.&lt;/p&gt; &lt;h3&gt;Deploy the aggregators&lt;/h3&gt; &lt;p&gt;Both applications can be deployed in the exact same manner as the enricher. Simply use &lt;code&gt;quay.io/evanshortiss/shipwars-streams-shot-distribution&lt;/code&gt; and &lt;code&gt;quay.io/evanshortiss/shipwars-streams-match-aggregates&lt;/code&gt; as the &lt;strong&gt;Image name&lt;/strong&gt; and add each to the &lt;code&gt;shipwars-analysis&lt;/code&gt; application that you created for the enricher application. View the logs once each application has started. Assuming you played a Shipwars match, you should immediately see logs stating that the aggregate shots record has been updated for a specific game ID.&lt;/p&gt; &lt;p&gt;Use the &lt;strong&gt;Open URL&lt;/strong&gt; button on the &lt;code&gt;shipwars-streams-shot-distribution&lt;/code&gt; node in the OpenShift Topology view to access the exposed route for the application. Append &lt;code&gt;/shot-distribution&lt;/code&gt; to the opened URL, in the following format:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;http://shipwars-streams-shot-distribution-$USERNAME-dev.$SUBDOMAIN.openshiftapps.com/shot-distribution&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The endpoint will return the aggregations in JSON format, as shown in Figure 5. The top-level keys are the game generation IDs. Each nested key represents a cell (X, Y coordinate) on the game grid, and contains the resulting hits and misses against that cell, classified by player type.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_5.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_5.jpg?itok=uYqD59WW" width="600" height="366" alt="An aggregation of shots for a given game server generation/deployment is shown in a JSON array, divided into hits and misses for the AI and the human player." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Aggregation of shots for a given game server generation. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Using the aggregated data&lt;/h3&gt; &lt;p&gt;This aggregated data could be used in various ways. The &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; AI move service could query it and adapt the game difficulty based on the hit rate of players. Alternatively, we could use the data to display a heatmap of shot activity, as you’ll see shortly.&lt;/p&gt; &lt;p&gt;You can view aggregated match records using the same approach. Click the &lt;strong&gt;Open URL&lt;/strong&gt; button on the &lt;code&gt;shipwars-streams-match-aggregate&lt;/code&gt; node in the OpenShift Topology view, and append &lt;code&gt;/replays&lt;/code&gt; to the URL to view match records.&lt;/p&gt; &lt;h2&gt;Analyzing attacks in real-time&lt;/h2&gt; &lt;p&gt;We use SmallRye and MicroProfile Reactive Streams, exposed by the &lt;code&gt;shipwars-streams-shot-distribution&lt;/code&gt; application, to create visualizations of the attacks on a real-time heatmap. The application is custom in this case, but it’s common to ingest the data into other analysis tools and systems using Kafka Connect.&lt;/p&gt; &lt;p&gt;You can test this endpoint by starting a cURL request using the following commands, and then playing the Shipwars game in your browser:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export AGGREGATOR_ROUTE=$(oc get route shipwars-streams-shot-distribution -o jsonpath='{.spec.host}') $ curl http://$AGGREGATOR_ROUTE/shot-distribution/stream&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your cURL request should print data similar to the result shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_6.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_6.jpg?itok=--uxaAa-" width="600" height="366" alt="A cURL command displays a stream of server-sent events." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Using cURL to display a stream of server-sent events. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Deploy the heatmap application&lt;/h3&gt; &lt;p&gt;Once you’re satisfied that the endpoint is working as expected, you can deploy the heatmap application. This is a web-based application that uses TypeScript, Tailwind CSS, and Parcel Bundler. We're using &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; to run the build tools, and NGINX serves the resulting HTML, CSS, and JavaScript.&lt;/p&gt; &lt;p&gt;Use the &lt;code&gt;oc new-app&lt;/code&gt; command to build the application on the Developer Sandbox via a &lt;a href="https://github.com/openshift/source-to-image"&gt;source-to-image&lt;/a&gt; process, and deploy the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# The builder/tools image export BUILDER=quay.io/evanshortiss/s2i-nodejs-nginx # Source code to build export SOURCE=https://github.com/evanshortiss/shipwars-visualisations # The public endpoint for the API exposing stream data export ROUTE=$(oc get route shipwars-streams-shot-distribution -o jsonpath='{.spec.host}') oc new-app $BUILDER~$SOURCE \ --name shipwars-visualisations \ --build-env STREAMS_API_URL=http://$ROUTE/ \ -l app.kubernetes.io/part-of=shipwars-analysis \ -l app.openshift.io/runtime=nginx&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new application shows up in the OpenShift Topology view immediately, but you’ll need to wait for the build to complete before the application becomes usable. Click the &lt;strong&gt;Open URL&lt;/strong&gt; button to view the application UI when the build has finished. A loading spinner is displayed initially, but once you start to play Shipwars in another browser window the heatmap will update in real-time, similar to the display in Figure 7.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_7.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_7.jpg?itok=jueBAeNw" width="600" height="366" alt="A heatmap, updated in real-time, shows how much each square has been attacked in Shipwars. Dark squares have been targeted by more attacks than light squares." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: A real-time heatmap shows how much each square has been attacked in Shipwars. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Squares that are targeted frequently will appear darker than squares that are targeted less often. Of course, our game has a low resolution of only 5x5, but in a game with larger maps (such as the first-person shooter games mentioned in the first half of this article) players would tend to spend more time in areas with better weapons and tactical cover. This would result in a useful heatmap for analyzing player behavior.&lt;/p&gt; &lt;h2&gt;Viewing the replays&lt;/h2&gt; &lt;p&gt;The interface we used to view replays at Red Hat Summit 2021 was created by Luke Dary. You can view the aggregated match replays by deploying the UI like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app quay.io/evanshortiss/shipwars-replay-ui \ -l "app.kubernetes.io/part-of=shipwars-analysis" \ -e REPLAY_SERVER="http://shipwars-streams-match-aggregates:8080" \ --name shipwars-replay $ oc expose svc shipwars-replay&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the application is deployed, you can view the replays by opening the application URL in a web browser. The replay UI is shown in Figure 8.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_8.png?itok=wG7F4Nhp" width="600" height="514" alt="Replays of the Shipwars match are aggregated by Kafka Streams." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The Shipwars match replays, aggregated by Kafka Streams. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you made it this far, well done! You’ve learned how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use the Red Hat OpenShift Application Services CLI to interact with OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Connect your OpenShift environment to your managed Kafka instances.&lt;/li&gt; &lt;li&gt;Deploy applications into an OpenShift environment using the OpenShift UI and CLI.&lt;/li&gt; &lt;li&gt;Use Kafka Streams to create data processing architectures with OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Using an OpenShift cluster and OpenShift Streams for Apache Kafka allows you to focus on building applications instead of infrastructure. Better yet, your applications can run anywhere and still utilize OpenShift Streams for Apache Kafka. The Shipwars application now includes &lt;a href="https://github.com/redhat-gamedev/shipwars-deployment#docker-compose"&gt;instructions for a Docker Compose&lt;/a&gt; deployment that can connect to the managed Kafka service. We suggest giving that a try next.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/31/game-telemetry-kafka-streams-and-quarkus-part-2" title="Game telemetry with Kafka Streams and Quarkus, Part 2"&gt;Game telemetry with Kafka Streams and Quarkus, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qjWAG2-BvTg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2021-08-31T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/31/game-telemetry-kafka-streams-and-quarkus-part-2</feedburner:origLink></entry><entry><title type="html">Quarkus 2.2.1.Final released - Hardening release</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/uMugHqAvPEo/" /><author><name>Guillaume Smet</name></author><id>https://quarkus.io/blog/quarkus-2-2-1-final-released/</id><updated>2021-08-31T00:00:00Z</updated><content type="html">Today we announce the availability of Quarkus 2.2.1.Final, which is the result of our first hardening cycle. Indeed, for 2.2, we decided to slow down on adding new features and focus this release cycle on hardening Quarkus with 3 main focuses: Fix issues Improve usability Improve documentation Thus the list...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/uMugHqAvPEo" height="1" width="1" alt=""/&gt;</content><dc:creator>Guillaume Smet</dc:creator><feedburner:origLink>https://quarkus.io/blog/quarkus-2-2-1-final-released/</feedburner:origLink></entry><entry><title>Automate Red Hat JBoss Web Server deployments with Ansible</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/lYvQvwWxVTc/automate-red-hat-jboss-web-server-deployments-ansible" /><author><name>Romain Pelisse</name></author><id>bb17fe33-34bd-4ea4-ae15-f2317bd13289</id><updated>2021-08-30T07:00:00Z</updated><published>2021-08-30T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/webserver/overview"&gt;Red Hat JBoss Web Server&lt;/a&gt; combines a web server (Apache HTTPD), a servlet engine (Apache Tomcat), and modules for load balancing (&lt;code&gt;mod_jk&lt;/code&gt; and &lt;code&gt;mod_cluster&lt;/code&gt;). Ansible is one of the best automation tools on the market. In this article, we'll use Ansible to completely automate the deployment of a JBoss Web Server instance on a freshly installed Red Hat Enterprise Linux (RHEL) server.&lt;/p&gt; &lt;p&gt;Our objective is to automate a JBoss Web Server deployment through the following tasks:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Retrieve the archive containing JBoss Web Server from a repository and install the files it contains on the system.&lt;/li&gt; &lt;li&gt;Configure the Red Hat Enterprise Linux operating system (create users, groups, and the required setup files to make JBoss Web Server a systemd service).&lt;/li&gt; &lt;li&gt;Fine-tune the configuration of the JBoss Web Server server itself (bind it to the appropriate interface and port).&lt;/li&gt; &lt;li&gt;Deploy a web application and starting the systemd service.&lt;/li&gt; &lt;li&gt;Perform a health check if the deployed application is accessible.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Ansible fully automates these operations, with no manual steps required.&lt;/p&gt; &lt;h2&gt;Set the target environment&lt;/h2&gt; &lt;p&gt;Before we start the automation, we need to specify our target environment. In this case, we're using RHEL 8 with Python 3.6.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# cat /etc/redhat-release Red Hat Enterprise Linux release 8.4 (Ootpa) # ansible --version ansible 2.9.22   config file = /etc/ansible/ansible.cfg   configured module search path = ['/root/.ansible/plugins/modules', '/usr/share/ansible/plugins/modules']   ansible python module location = /usr/lib/python3.6/site-packages/ansible   executable location = /usr/bin/ansible   python version = 3.6.8 (default, Mar 18 2021, 08:58:41) [GCC 8.4.1 20200928 (Red Hat 8.4.1-1)]&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The playbook might not work if you want to use a different Python version or target operating system.&lt;/p&gt; &lt;h2&gt;Install the JBoss Web Server Ansible collection&lt;/h2&gt; &lt;p&gt;Once you have RHEL 8 set up and Ansible ready to go, you need to install the &lt;a href="https://galaxy.ansible.com/middleware_automation/jws"&gt;JBoss Web Server Ansible collection&lt;/a&gt;. Ansible uses the collection to perform the following tasks on JBoss Web Server:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Ensure required dependencies are installed (e.g., unzip).&lt;/li&gt; &lt;li&gt;Install Java (if missing and requested).&lt;/li&gt; &lt;li&gt;Install the binaries and integrate the software into the system (user, group, etc.).&lt;/li&gt; &lt;li&gt;Deploy the configuration files.&lt;/li&gt; &lt;li&gt;Start and enable JBoss Web Server as a systemd service.&lt;/li&gt; &lt;li&gt;Configure a Password Vault with JBoss Web Server and a load balancer (e.g., &lt;code&gt;mod_cluster&lt;/code&gt;)—not used for this article.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Here's the installation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible-galaxy collection install middleware_automation.jws Process install dependency map Starting collection install process Installing 'middleware_automation.jws:0.0.1' to '/root/.ansible/collections/ansible_collections/middleware_automation/jws' Installing 'middleware_automation.redhat_csp_download:1.1.2' to '/root/.ansible/collections/ansible_collections/middleware_automation/redhat_csp_download'&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: &lt;a href="https://galaxy.ansible.com/"&gt;Ansible Galaxy&lt;/a&gt; fetches and downloads the collection's dependencies. That is why, at the end of the execution, the JBoss Web Server collection was installed, as well as &lt;code&gt;redhat_csp_download&lt;/code&gt;, which will help facilitate the retrieval of the archive containing the JBoss Web Server server.&lt;/p&gt; &lt;p&gt;Installing the collection reduces the configuration to achieve our automation to the bare minimum.&lt;/p&gt; &lt;h2&gt;Create a playbook to test the installation&lt;/h2&gt; &lt;p&gt;Before we continue, let's create a minimal playbook to confirm that the collection was properly installed:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   collections: – middleware_automation.jws   tasks :&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In its current state, this playbook doesn’t perform any tasks on the target system. If the playbook runs successfully, then we know that the collection has been properly installed.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -i hosts min.yml PLAY [JBoss Web Server installation and configuration] ************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost              : ok=1 changed=0 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Install the Apache Tomcat web server&lt;/h2&gt; &lt;p&gt;Next, we'll install the Apache Tomcat web server, which consists of several steps.&lt;/p&gt; &lt;h3&gt;Download the archive&lt;/h3&gt; &lt;p&gt;First, let's modify our minimal playbook to retrieve the archive containing Apache Tomcat. For this purpose, we will leverage the &lt;code&gt;get_url&lt;/code&gt; module from Ansible:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: tomcat_version: 9.0.50 tomcat_download_url: https://archive.apache.org/dist/tomcat/tomcat-9/v{{tomcat_version}}/bin/apache-tomcat-{{tomcat_version}}.zip tomcat_install_dir: /opt tomcat_zipfile: “{{tomcat_install_dir}}/tomcat.zip"   collections: - middleware_automation.jws pre_tasks: - name: "Download latest JBoss Web Server Zipfile from {{ tomcat_download_url }}."    get_url:      url: "{{ tomcat_download_url }}"      dest: "{{ tomcat_zipfile }}"             remote_src: yes    when:      - tomcat_download_url is defined&lt;/code&gt;&lt;/pre&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: The playbook downloads the archive during the &lt;code&gt;pre_tasks&lt;/code&gt; section. It’s important to use the role provided by the JBoss Web Server collection in the next step. This role will be executed before the &lt;code&gt;tasks&lt;/code&gt; block and after the &lt;code&gt;pre_tasks&lt;/code&gt; block, and it requires that the archive file be already present on the target system.&lt;/p&gt; &lt;p&gt;Execute a new playbook:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# ansible-playbook -i hosts playbook.yml PLAY [JBoss Web Server installation and configuration] ************************************************************************* TASK [Gathering Facts] ***************************************************************************************************************** ok: [localhost] TASK [Download latest JBoss Web Server Zipfile from https://archive.apache.org/dist/tomcat/tomcat-9/v9.0.50/bin/apache-tomcat-9.0.50.zip.] *** changed: [localhost] PLAY RECAP ***************************************************************************************************************************** localhost              : ok=2 changed=1 unreachable=0 failed=0 skipped=0 rescued=0 ignored=0&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Java&lt;/h3&gt; &lt;p&gt;JBoss Web Server is a Java-based server, so the target system is required to install a Java Virtual Machine (JVM). While Ansible primitives can perform such tasks natively, the &lt;code&gt;jws&lt;/code&gt; role can also take care of this part, provided the &lt;code&gt;tomcat_java_version&lt;/code&gt; variable is defined:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;- name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: ... tomcat_java_version: 1.8.0   collections: – middleware_automation.jws …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Keep in mind that this feature has limits; it works only if the target system’s distribution belongs to the Red Hat &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_conditionals.html#ansible-facts-os-family"&gt;family&lt;/a&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ ansible -m setup localhost | grep family      "ansible_os_family": "RedHat",&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Install Java web server&lt;/h3&gt; &lt;p&gt;For Java web server to work, we need to provide one more variable to our playbook. The &lt;code&gt;tomcat_setup&lt;/code&gt; (set to &lt;code&gt;true&lt;/code&gt;) signals to the &lt;code&gt;jws&lt;/code&gt; role that we want it to perform the installation:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: tomcat_setup: true ...   collections: - middleware_automation.jws   roles: - jws   pre_tasks: ...   tasks:&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Run the playbook again&lt;/h2&gt; &lt;p&gt;Let’s &lt;a href="https://gist.github.com/rpelisse/7e97b53c080c5a14ea601cf6ba5cc8bc"&gt;run our playbook again&lt;/a&gt; to see if it works as expected.&lt;/p&gt; &lt;p&gt;As you can see, quite a lot happened during this execution. Indeed, the &lt;code&gt;jws&lt;/code&gt; role took care of all the setup:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Deploying a base configuration.&lt;/li&gt; &lt;li&gt;Removing unused applications.&lt;/li&gt; &lt;li&gt;Starting the web server.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The playbooks also perform a few tasks on the target system, like installing any required dependencies if they are missing and ensuring the environment is properly configured.&lt;/p&gt; &lt;h2&gt;Configure JBoss Web Server as a systemd service&lt;/h2&gt; &lt;p&gt;A nice feature of the &lt;code&gt;jws&lt;/code&gt; role is the included functionality to configure JBoss Web Server as a systemd service. For this to occur, you just need to define the &lt;code&gt;tomcat_service_name&lt;/code&gt; variable:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;--- - name: "JBoss Web Server installation and configuration"   hosts: "all"   become: yes   vars: … tomcat_service_name: tomcat ...   collections: - middleware_automation.jws   roles:      …&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Keep in mind this only works when systemd is installed and the system belongs to the Red Hat family.&lt;/p&gt; &lt;pre&gt; &lt;code&gt;# systemctl status tomcat ● tomcat.service - JBoss Web Server Web Application Container   Loaded: loaded (/usr/lib/systemd/system/tomcat.service; enabled; vendor preset: disabled)   Active: active (running) since Thu 2021-07-22 16:52:08 UTC; 16h ago    Main PID: 6234 (java)    Tasks: 37 (limit: 307)   Memory: 187.4M      CPU: 21.528s   CGroup: /system.slice/tomcat.service └─6234 /usr/bin/java -Djava.util.logging.config.file=/opt/apache-tomcat-9.0.50/conf/logging.properties -Djava.util.logging.manager=org.apache.juli.ClassLoaderLogManager -Djdk.tls.ephemeralDHKeySize=2048 -Djava.protocol.handler.pkgs=org.apache.catalina.webresources -Dorg.apache.catalina.security.SecurityListener.UMASK=0027 -Dignore.endorsed.dirs= -classpath /opt/apache-tomcat-9.0.50/bin/bootstrap.jar:/opt/apache-tomcat-9.0.50/bin/tomcat-juli.jar -Dcatalina.base=/opt/apache-tomcat-9.0.50 -Dcatalina.home=/opt/apache-tomcat-9.0.50 -Djava.io.tmpdir=/opt/apache-tomcat-9.0.50/temp org.apache.catalina.startup.Bootstrap start Jul 22 16:52:08 4414af93c931 systemd[1]: Started Apache Tomcat Web Application Container. Jul 22 16:52:08 4414af93c931 systemd-service.sh[6220]: Tomcat started. Jul 22 16:52:08 4414af93c931 systemd-service.sh[6219]: Tomcat runs with PID: 6234&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Deploy a web application&lt;/h2&gt; &lt;p&gt;Now that JBoss Web Server is running, let’s modify the playbook and facilitate the deployment of a web application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;tasks:     - name: " Checks that server is running"       uri:         url: "http://localhost:8080/"         status_code: 404         return_content: no     - name: "Deploy demo webapp"       get_url:         url: 'https://people.redhat.com/~rpelisse/info-1.0.war'         dest: "{{ tomcat_home }}/webapps/info.war"       notify:         - Restart Tomcat service&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;A &lt;a href="https://docs.ansible.com/ansible/latest/user_guide/playbooks_handlers.html"&gt;handler&lt;/a&gt; in the &lt;code&gt;jws&lt;/code&gt; role restarts JBoss Web Server when the web application is downloaded. To finish our demonstration, we can add a quick test in the &lt;code&gt;post_tasks&lt;/code&gt; section of the playbook to confirm that the web application is functional:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;  post_tasks: - name: "Sleep for {{ tomcat_sleep }} seconds to let Tomcat starts "    wait_for:      timeout: "{{ tomcat_sleep }}" - name: "Test application"    get_url:      url: "http://localhost:8080/info/"      dest: /tmp/info.txt&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;That’s all for today! Future articles will demonstrate other features provided by the Red Hat JBoss Web Server collection, including support for &lt;code&gt;mod_cluster&lt;/code&gt; and securing the server with Tomcat’s Vault feature.&lt;/p&gt; &lt;p&gt;In the meantime, you can find the playbook used for this article in the &lt;a href="https://github.com/ansible-middleware/jws-ansible-playbook/blob/trunk/playbook.yml"&gt;Ansible Collection for JBoss Web Server GitHub repository&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible" title="Automate Red Hat JBoss Web Server deployments with Ansible"&gt;Automate Red Hat JBoss Web Server deployments with Ansible&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/lYvQvwWxVTc" height="1" width="1" alt=""/&gt;</summary><dc:creator>Romain Pelisse</dc:creator><dc:date>2021-08-30T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/30/automate-red-hat-jboss-web-server-deployments-ansible</feedburner:origLink></entry></feed>
