<?xml version="1.0" encoding="UTF-8"?>
<?xml-stylesheet type="text/xsl" media="screen" href="/~d/styles/atom10full.xsl"?><?xml-stylesheet type="text/css" media="screen" href="http://feeds.feedburner.com/~d/styles/itemcontent.css"?><feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/" xmlns:feedburner="http://rssnamespace.org/feedburner/ext/1.0"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="self" type="application/atom+xml" href="http://feeds.feedburner.com/jbossbuzz" /><feedburner:info uri="jbossbuzz" /><atom10:link xmlns:atom10="http://www.w3.org/2005/Atom" rel="hub" href="http://pubsubhubbub.appspot.com/" /><entry><title>Build a Kubernetes Operator in six steps</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/1QJ_EThzyfI/build-kubernetes-operator-six-steps" /><author><name>Deepak Sharma</name></author><id>77edefc1-7afc-4f6d-86a8-ac9d38ee6c44</id><updated>2021-09-07T03:00:00Z</updated><published>2021-09-07T03:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/kubernetes/operators"&gt;Operators&lt;/a&gt; greatly increase the power of &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; as an environment and orchestration tool for running scalable applications This article shows you how to create your own Kubernetes Operator. Although production applications often run in the cloud, you don't need a cloud service for the tutorial; you'll download everything you need onto a local system.&lt;/p&gt; &lt;p&gt;This article is an update to one I wrote last year, &lt;a href="https://developers.redhat.com/blog/2020/08/21/hello-world-tutorial-with-kubernetes-operators"&gt;'Hello, World' tutorial with Kubernetes Operators&lt;/a&gt;. Architecture upgrades in the Kubernetes Operator SDK (in version 0.20) put that article out of date. This tutorial takes you on the journey of writing your first Kubernetes Operator using Kubernetes Operator SDK 1.11+.&lt;/p&gt; &lt;h2&gt;The role and behavior of Kubernetes Operators&lt;/h2&gt; &lt;p&gt;A Kubernetes Operator manages your application's logistics. It contains code called a &lt;em&gt;controller&lt;/em&gt; that runs periodically and checks the current state of your service's namespaced resources against the desired state. If the controller finds any differences, it restores your service to the desired state in a process called &lt;em&gt;reconciliation&lt;/em&gt;. For instance, if a resource crashed, the controller restarts it.&lt;/p&gt; &lt;p&gt;You can imagine an unofficial agreement between you and the Kubernetes Operator:&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;You&lt;/strong&gt;: "Hey Opo, I am creating the following resources. Now it's your responsibility to keep them running."&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Operator&lt;/strong&gt;: "Roger that! Will check back regularly."&lt;/p&gt; &lt;p&gt;You can build an operator with Helm Charts, Ansible playbooks, or Golang. In this article, we use Golang. We'll focus on a namespace-scoped operator (as opposed to a cluster-scoped operator) because it's more flexible, and because we want to control only our own application. See the &lt;a href="https://developers.redhat.com/articles/2021/06/11/kubernetes-operators-101-part-1-overview-and-key-features"&gt;Kubernetes Operators 101&lt;/a&gt; series for more background on operators.&lt;/p&gt; &lt;p&gt;Now we'll create an operator. Let's go ...&lt;/p&gt; &lt;h2&gt;Setup and prerequisites&lt;/h2&gt; &lt;p&gt;We'll start by getting these resources on your system:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;The &lt;a href="https://golang.org/dl/"&gt;Go&lt;/a&gt; language (Golang)&lt;/li&gt; &lt;li&gt;The &lt;a href="https://minikube.sigs.k8s.io/docs/start/"&gt;Minikube&lt;/a&gt; environment for running Kubernetes locally&lt;/li&gt; &lt;li&gt;The &lt;a href="https://v1-11-x.sdk.operatorframework.io/docs/installation/"&gt;Kubernetes Operator SDK 1.11+&lt;/a&gt;&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;For prerequisites, I recommend the following&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Some programming language experience. This example uses Golang, so some knowledge of that language would be helpful but is not required.&lt;/li&gt; &lt;li&gt;Patience (very important).&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Let's set up the software requirements.&lt;/p&gt; &lt;h3&gt;Installing Golang&lt;/h3&gt; &lt;p&gt;Get Golang from the &lt;a href="https://golang.org/dl/"&gt;Golang download site&lt;/a&gt;, then configure the following environment variable:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$GOPATH=/your/preferred/path/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Next, verify the installation:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Verify $ go version go version go1.16.3 linux/amd64 &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Setting up a cluster on Minikube&lt;/h3&gt; &lt;p&gt;After downloading &lt;a href="https://minikube.sigs.k8s.io/docs/start/"&gt;Minikube&lt;/a&gt;, make sure it is properly installed and running:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ minikube status minikube type: Control Plane host: Running kubelet: Running apiserver: Running kubeconfig: Configured&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Setting up the Operator SDK&lt;/h3&gt; &lt;p&gt;&lt;a href="https://sdk.operatorframework.io/docs/installation/"&gt;Install the Operator SDK&lt;/a&gt;, then verify that it is installed:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# Verify $ operator-sdk version operator-sdk version: "v1.11.0", commit: "28dcd12a776d8a8ff597e1d8527b08792e7312fd", kubernetes version: "1.20.2", go version: "go1.16.7", GOOS: "linux", GOARCH: "amd64" &lt;/code&gt;&lt;/pre&gt; &lt;h2 id="build_the_operator-h2"&gt;Build the Kubernetes Operator&lt;/h2&gt; &lt;p&gt;Now we'll build our Kubernetes Operator in just six steps. I provide a link to the code for you to download at each step.&lt;/p&gt; &lt;h3&gt;Step 1: Generate boilerplate code&lt;/h3&gt; &lt;p&gt;To create your local cluster, run the &lt;code&gt;minikube start&lt;/code&gt; command:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ mkdir -p $GOPATH/src/operators &amp;&amp; cd $GOPATH/src/operators $ minikube start init&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Then run &lt;code&gt;operator-sdk init&lt;/code&gt; to generate the boilerplate code for our example application:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ operator-sdk init&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;a href="https://github.com/deepak1725/hello-operator2/commit/d37dd4ea3035a26889d3028d58e7f2da1220a5cb" target="_blank"&gt;code for this step&lt;/a&gt; is available in my Hello Operator GitHub repository.&lt;/p&gt; &lt;h3&gt;Step 2: Create APIs and a custom resource&lt;/h3&gt; &lt;p&gt;In Kubernetes, the functions exposed for each service you want to provide are grouped together in a &lt;em&gt;resource&lt;/em&gt;. Thus, when we create the APIs for our application, we also create their resource through a &lt;a href="https://access.redhat.com/documentation/en-us/openshift_container_platform/3.7/html/cluster_administration/admin-guide-custom-resources"&gt;CustomResourceDefinition&lt;/a&gt; (CRD).&lt;/p&gt; &lt;p&gt;The following command creates an API and labels it &lt;code&gt;Traveller&lt;/code&gt; through the &lt;code&gt;--kind&lt;/code&gt; option. In the YAML configuration files created by the command, you can find a field labeled &lt;code&gt;kind&lt;/code&gt; with the value &lt;code&gt;Traveller&lt;/code&gt;. This field indicates that &lt;code&gt;Traveller&lt;/code&gt; is used throughout the development process to refer to our APIs:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ operator-sdk create api --version=v1alpha1 --kind=Traveller Create Resource [y/n] y Create Controller [y/n] y ... ...&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We have asked the command also to create a controller to handle all operations corresponding to our &lt;code&gt;kind&lt;/code&gt;. The file defining the controller is named &lt;code&gt;traveller_controller.go&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;--version&lt;/code&gt; option can take any string, and you can set it to track your development on a project. Here, we've started with a modest value, indicating that our application is in alpha.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/deepak1725/hello-operator2/commit/5d18c604bf82be02f4bcfa49f12c33541b704d92" target="_blank"&gt;code for this step&lt;/a&gt; is available in the Hello Operator GitHub repository.&lt;/p&gt; &lt;h3&gt;Step 3: Download the dependencies&lt;/h3&gt; &lt;p&gt;Our application uses the &lt;code&gt;tidy&lt;/code&gt; module to remove dependencies we don't need, and the &lt;code&gt;vendor&lt;/code&gt; module &lt;a href="https://golang.org/ref/mod#vendoring"&gt;to consolidate packages&lt;/a&gt;. Install these modules as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ go mod tidy $ go mod vendor&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;a href="https://github.com/deepak1725/hello-operator2/commit/98e4ebc8532696b1f65e5471fd49b1b54c7f1031" target="_blank"&gt;code for this step&lt;/a&gt; is available in the Hello Operator GitHub repository.&lt;/p&gt; &lt;h3&gt;Step 4: Create a deployment&lt;/h3&gt; &lt;p&gt;Now we will create, under our Kubernetes Operator umbrella, the standard resources that make up a containerized application. Because a Kubernetes Operator runs iteratively to reconcile the state of your application, it's very important to write the controller to be idempotent: In other words, the controller can run the code multiple times without creating multiple instances of a resource.&lt;/p&gt; &lt;p&gt;The following repo includes a controller for a deployment resource in the file &lt;code&gt;controllers/deployment.go&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/deepak1725/hello-operator2/commit/c87544405b8359da2007317112bb64e434330f5f" target="_blank"&gt;code for this step&lt;/a&gt; is available in the Hello Operator GitHub repository.&lt;/p&gt; &lt;h3&gt;Step 5: Create a service&lt;/h3&gt; &lt;p&gt;Because we want the pods created by our deployment to be accessible outside our system, we attach a service to the deployment we just created. The code is in the file &lt;code&gt;controllers/service.go&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;The &lt;a href="https://github.com/deepak1725/hello-operator2/commit/b17c24f7abd75328aa1549b0afb6c6bcb1ca8f61" target="_blank"&gt;code for this step&lt;/a&gt; is available in the Hello Operator GitHub repository.&lt;/p&gt; &lt;h3&gt;Step 6: Add a reference in the controller&lt;/h3&gt; &lt;p&gt;This step lets our controller know the existence of the deployment and service. It does this through edits to the reconciliation loop function of the &lt;code&gt;traveller_controller.go&lt;/code&gt; file.&lt;/p&gt; &lt;p&gt;Find the &lt;a href="https://github.com/deepak1725/hello-operator2/commit/324a56116aa561bf44b34402287edbfa87131cc6" target="_blank"&gt;code for this step&lt;/a&gt; in the Hello Operator GitHub repository.&lt;/p&gt; &lt;p&gt;Now, perhaps it's time for a hydration break. Then we'll try out our service.&lt;/p&gt; &lt;h2&gt;Deploy the service&lt;/h2&gt; &lt;p&gt;There are multiple ways to deploy our CRD:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Run the server locally.&lt;/li&gt; &lt;li&gt;Run the server in a cluster.&lt;/li&gt; &lt;li&gt;Deploy the service via an &lt;a href="https://developers.redhat.com/blog/2021/02/08/deploying-kubernetes-operators-with-operator-lifecycle-manager-bundles"&gt;Operator Lifecycle Manager (OLM) bundle&lt;/a&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;For the sake of brevity, we will run the service locally.&lt;/p&gt; &lt;h3&gt;Installing the CRD&lt;/h3&gt; &lt;p&gt;All we have to do to deploy our hard work locally is to run a build:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ make install&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command registers our custom kind schema (&lt;code&gt;Traveller&lt;/code&gt; in this case) within our Kubernetes cluster. Now any new request specifying this kind will be forwarded to our &lt;code&gt;Traveller&lt;/code&gt; controller internally.&lt;/p&gt; &lt;p&gt;You will find the &lt;a href="https://github.com/deepak1725/hello-operator2/commit/31dc35d73b2f2a89b7e1868a4c04bb4012fc66f4" target="_blank"&gt;code for this step&lt;/a&gt; in the Hello Operator GitHub repository.&lt;/p&gt; &lt;h3&gt;Deploying a CRD instance&lt;/h3&gt; &lt;p&gt;We still have to enable our resource in Kubernetes. Queue up a request to create our resource through the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kustomize build config/samples | kubectl apply -f -&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;At this stage, our Kubernetes cluster is aware of our &lt;code&gt;Traveller&lt;/code&gt; CRD. Spin up the controller:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ make run&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command will execute the reconciliation function in &lt;code&gt;traveller_controller.go&lt;/code&gt;, which in turn creates our deployment and service resources.&lt;/p&gt; &lt;h2&gt;Run the Kubernetes Operator&lt;/h2&gt; &lt;p&gt;Now we will dive into our local cluster and check its behavior.&lt;/p&gt; &lt;h3&gt;Checking the state&lt;/h3&gt; &lt;p&gt;Make sure that the resources are running:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;$ kubectl get all NAME READY STATUS RESTARTS AGE pod/hello-pod-6bbd776b6d-cxp46 1/1 Running 0 6m4s NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE service/backend-service NodePort 10.xx.xxx.xxx &lt;none&gt; 80:30685/TCP 6m4s service/kubernetes ClusterIP 10.xx.0.1 &lt;none&gt; 443/TCP 168m NAME READY UP-TO-DATE AVAILABLE AGE deployment.apps/hello-pod 1/1 1 1 6m4s NAME DESIRED CURRENT READY AGE replicaset.apps/hello-pod-6bbd776b6d 1 1 1 6m4s &lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Exposing the service&lt;/h3&gt; &lt;p&gt;Open our newly created service in a browser as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ minikube service backend-service |-----------|-----------------|-------------|---------------------------| | NAMESPACE |      NAME       | TARGET PORT |            URL            | |-----------|-----------------|-------------|---------------------------| | default   | backend-service |          80 | http://192.168.49.2:30685 | |-----------|-----------------|-------------|---------------------------| ? Opening service default/backend-service in default browser... ➜ ~ Opening in existing browser session.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The browser screen looks like Figure 1. Congratulations—you have just deployed your first Kubernetes Operator.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/hello_screen.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/hello_screen.png?itok=6PCXCplB" width="600" height="158" alt="When fully deployed, the service shows a Hello World screen." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The screen displayed by running a service. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;Operators extend Kubernetes APIs and create custom objects in the cluster. This feature of Kubernetes opens a number of avenues for developers to customize the cluster in a manner best suited for our application and environment. This article only touched on the power of Kubernetes Operators in a minimal way. They are capable of doing far more than what we accomplished here. I encourage you to take this article as a starting point for building awesome Kubernetes Operators.&lt;/p&gt; &lt;p&gt;I hope you enjoyed the journey.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/07/build-kubernetes-operator-six-steps" title="Build a Kubernetes Operator in six steps"&gt;Build a Kubernetes Operator in six steps&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/1QJ_EThzyfI" height="1" width="1" alt=""/&gt;</summary><dc:creator>Deepak Sharma</dc:creator><dc:date>2021-09-07T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/07/build-kubernetes-operator-six-steps</feedburner:origLink></entry><entry><title>Improving CI/CD in Red Hat OpenShift</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/_jGHs2l6LJg/improving-cicd-red-hat-openshift" /><author><name>Brigid Gliwa</name></author><id>82c3df91-7d61-4564-847b-a599f5f2a9ad</id><updated>2021-09-06T03:00:00Z</updated><published>2021-09-06T03:00:00Z</published><summary type="html">&lt;p&gt;Red Hat recently conducted a &lt;a href="https://cloud.redhat.com/blog/introduction-to-customer-empathy-workshops"&gt;Customer Empathy Workshop series&lt;/a&gt; that included two virtual workshops focused on continuous integration and continuous delivery (&lt;a href="https://developers.redhat.com/topics/ci-cd"&gt;CI/CD&lt;/a&gt;) tooling within &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt;. The Red Hat User Experience Design team partnered with Product Management and Engineering to engage 22 OpenShift customers from seven different companies.&lt;/p&gt; &lt;p&gt;During each workshop, we used digital whiteboards and video conferencing to virtually engage customers in hands-on activities. We used the first three steps of the classic design thinking—empathize, define, and ideate—to identify and understand problems as well as suggest solutions. Let’s take a look at what we learned.&lt;/p&gt; &lt;h2&gt;Problems and pain points with CI/CD&lt;/h2&gt; &lt;p&gt;We began the workshops by asking participants to introduce themselves in order to help break the ice and get to know each other. Each customer shared their role and responsibilities, the CI/CD tools they use, and what they were looking forward to in the workshop. After everyone got the chance to share, we dove into the first step of design thinking: Empathize. Although we couldn’t use sticky notes and Sharpies as we do in person, participants were able to use digital whiteboarding to write down their problems and pain points when they use CI/CD tooling within OpenShift. We reviewed the participants’ pain points to find similarities and identify common themes.&lt;/p&gt; &lt;p&gt;Figure 1 lists some of those themes, followed by a list that includes paraphrased responses from customers.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Bucket%20stacks.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Bucket%20stacks.png?itok=neW92pmj" width="600" height="596" alt="Nine themes were raised by customers in the design workshop." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Themes from design workshop. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;Tekton limitations&lt;/strong&gt;: "Passing data through a Tekton Pipeline is cumbersome and chatty. Tekton also lacks the capability for human confirmation in pipelines."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Argo CD&lt;/strong&gt;: "We need better integration of Argo CD, Helm, and Tekton."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multitenancy&lt;/strong&gt;: "Multitenancy and Kubernetes RBAC should be enforced in tools."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Pipeline as code&lt;/strong&gt;: "Our developers would like simple pipeline definitions, such as GitHub Actions or Travis. It is difficult to share pipelines or parts of the pipeline."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Metrics and gates:&lt;/strong&gt; "We are unable to track test results across consecutive runs. OpenShift lacks the ability to set up gates for the pipeline."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Best practices&lt;/strong&gt;: "What is Red Hat's ideal CI/CD architecture? How could we get feedback from OpenShift in our CI/CD tooling?"&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Triggers&lt;/strong&gt;: "Tekton EventListeners are not protected with authentication or OAuth. EventListeners also can't override default parameters of pipelines."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Delivering secrets&lt;/strong&gt;: "There is no secret handling in Argo CD or OpenShift GitOps and integrating with an external vault is not easy. Secrets do not work well in resource-constrained clusters."&lt;/li&gt; &lt;li&gt;&lt;strong&gt;Multicluster management&lt;/strong&gt;: "We need Git repo management for Argo CD and synching of multiple clusters, along with configuration as code."&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;How might we improve CI/CD?&lt;/h2&gt; &lt;p&gt;The next step in the design thinking process is to define the problem. In this step, we grouped our issues into different themes and selected a few to focus on for the remainder of the workshop. We used these key themes to define problem statements in the format of "How might we" statements. Reframing the problem into a "How might we" statement helped to shift our perspective from challenges to opportunities. Figure 2 shows the problem statements we created and groups some participant observations within each statement.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Pain%20point%20graphic_0.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Pain%20point%20graphic_0.png?itok=gU4pKeCD" width="600" height="550" alt="For five areas identified as "pain points," several projects were found that could improve them." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Pain points and projects that can address them. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The statements are as follows:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;How might we make it easier to onboard and use Tekton Pipelines effectively?&lt;/li&gt; &lt;li&gt;How might we make it easier to learn how to leverage Argo CD effectively?&lt;/li&gt; &lt;li&gt;How might we share parts of pipelines more easily?&lt;/li&gt; &lt;li&gt;How might we offer a flexible way to customize pipeline metrics and react to them?&lt;/li&gt; &lt;li&gt;How might we get more information about what Red Hat thinks is the right way to use the tools?&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Ideas for improving CI/CD&lt;/h2&gt; &lt;p&gt;The final step of the design thinking process in these workshops was to ideate new solutions to the problem statements we created. We used the "Yes, and" technique to facilitate brainstorming. The phrase "Yes, and" encourages active listening, positive thinking, and building on each other’s ideas. It reminds people not to criticize other people's ideas, but to use the ideas as inspiration for more suggestions. This process generated many possible solutions to the problems we were exploring, as summarized in Figure 3.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/Solutions.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/Solutions.png?itok=U66RHv9u" width="600" height="287" alt="Ideating finds positive steps an organization can take to solve a problem." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Suggestions resulting from ideating. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Let's look at our findings from each of the problem statements.&lt;/p&gt; &lt;h3&gt;How might we make it easier to onboard and use Tekton Pipelines effectively?&lt;/h3&gt; &lt;p&gt;Participants generated the following ideas for making Tekton Pipelines onboarding easier:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Provide additional information from elsewhere (secrets, registry, etc) during the pipeline creation process.&lt;/li&gt; &lt;li&gt;Provide an out-of-the-box way of protecting Triggers and EventListeners.&lt;/li&gt; &lt;li&gt;Provide additional metrics on a per-pipeline level (success ratio, run duration, etc.).&lt;/li&gt; &lt;li&gt;Create a pipeline template that generates pipelines based on each folder.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;How might we make it easier to learn how to leverage Argo CD effectively?&lt;/h3&gt; &lt;p&gt;Ideas for learning how to leverage Argo CD were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Integrate better with external vaults (e.g., HashiCorp).&lt;/li&gt; &lt;li&gt;Document complex use cases.&lt;/li&gt; &lt;li&gt;Write guided docs to achieve use cases (after setup).&lt;/li&gt; &lt;li&gt;Implement secrets via Argo CD or Vault.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;How might we share parts of pipelines more easily?&lt;/h3&gt; &lt;p&gt;For sharing parts of pipelines, participants had the following suggestions:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use a simpler syntax (e.g., GitHub actions).&lt;/li&gt; &lt;li&gt;Make sure that every time you have a new branch, the pipeline stays the same and certain tasks are skipped based on the branch.&lt;/li&gt; &lt;li&gt;Save users from having to think about the technical details of where the pipeline is running, and how many resources it is using.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;How might we offer a flexible way to customize pipeline metrics and react to them?&lt;/h3&gt; &lt;p&gt;Ideas for providing more flexible pipeline-metric customization were:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Allow custom metrics to be defined at certain author-defined thresholds that will fail the pipeline.&lt;/li&gt; &lt;li&gt;Make results of performance scans processable: if they are above a certain value they should block or break the pipeline.&lt;/li&gt; &lt;li&gt;Provide a yes-or-no button: Should I proceed with the next step?&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;How might we get more information about what Red Hat thinks is the right way to do things and use the tools?&lt;/h3&gt; &lt;p&gt;Participants suggested the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Define a reference architecture.&lt;/li&gt; &lt;li&gt;Publish ideas from Red Hat about how to use the tools—we don’t want to search the internet.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Finally, we voted on the solutions participants most wanted to see implemented. This information will help the OpenShift team prioritize new features and enhancements for CI/CD tooling in the console.&lt;/p&gt; &lt;h2&gt;What’s next&lt;/h2&gt; &lt;p&gt;The User Experience Design team will continue to explore the customer ideas that came from these workshops. We will complete the design thinking process by prototyping designs and testing them with users. The information that we gathered from these two workshops helped us to identify five main focus areas that will inform decisions and enhancements to the OpenShift CI/CD tooling.&lt;/p&gt; &lt;p&gt;Community feedback helps us continually improve the OpenShift Developer Experience. We want to hear from you, too. Attend one of our office hours on the OpenShift Twitch channel, or join the OpenShift Developer Experience Google Group to share your Web Console tips, get help with what doesn’t work so well for you, and shape the future of the OpenShift Developer Experience.&lt;/p&gt; &lt;p&gt;Ready to get started? &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Try OpenShift today&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Are you interested in being a user research participant? &lt;a href="https://redhatdg.co1.qualtrics.com/jfe/form/SV_7aLuDILtNL7FmPX?source=OpenShift-blog"&gt;Sign up today&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/06/improving-cicd-red-hat-openshift" title="Improving CI/CD in Red Hat OpenShift"&gt;Improving CI/CD in Red Hat OpenShift&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/_jGHs2l6LJg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Brigid Gliwa</dc:creator><dc:date>2021-09-06T03:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/06/improving-cicd-red-hat-openshift</feedburner:origLink></entry><entry><title type="html">Byteman 4.0.17 has been released</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/rWbhPy3hdCQ/byteman-4017-has-been-released.html" /><author><name>Andrew Dinn</name></author><id>http://bytemanblog.blogspot.com/2021/09/byteman-4017-has-been-released.html</id><updated>2021-09-03T14:47:00Z</updated><content type="html">Byteman 4.0.17 is now available from the and from the . It is the latest update release for use on all JDK9+ runtimes up to and including JDK17.   Byteman 4.0.17 is a maintenance release which provides a few small enhancements and fixes a minor bug. More details are provided in the .  &lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/rWbhPy3hdCQ" height="1" width="1" alt=""/&gt;</content><dc:creator>Andrew Dinn</dc:creator><feedburner:origLink>http://bytemanblog.blogspot.com/2021/09/byteman-4017-has-been-released.html</feedburner:origLink></entry><entry><title type="html">Bringing Drools rules into the cloud with Kogito: a step by step path</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/GDkJ8DmV0lM/bringing-drools-rules-into-the-cloud-with-kogito-a-step-by-step-path.html" /><author><name>Mario Fusco</name></author><id>https://blog.kie.org/2021/09/bringing-drools-rules-into-the-cloud-with-kogito-a-step-by-step-path.html</id><updated>2021-09-02T07:20:36Z</updated><content type="html">The goal of this article is to demonstrate how to expose a Drools stateless rules evaluation in a Quarkus REST endpoint and then how to migrate it to Kogito in order to fully leverage Quarkus features and finally how to embrace the Kogito’s programming model based on rule units. OVERVIEW In Drools a stateless rules evaluation is a one-off execution of a rule set against a provided set of facts. Under this point of view this kind of rules evaluation can be seen as a pure function invocation, where the return value is only determined by its input values, without observable side effects, in which the arguments passed to the function are actually the facts to be inserted into the session and the result is the outcome of your rules set applied on those facts. From a consumer perspective of this service the fact that the invoked function uses a rule engine to perform its job could be only an internal implementation detail. In this situation it is natural to expose such a function through a REST endpoint, thus turning it into a microservice. At this point it can be eventually deployed into a Function as a Service environment, possibly after having compiled it into a native image, to avoid paying the cost of relatively high JVM startup time. This document is focused on this stateless scenario because at the moment it is the only use case also supported in Kogito. THE SAMPLE PROJECT Let’s try to put this idea in practice by taking an existing Drools project and migrating it in steps to Kogito and Quarkus. The domain model of the sample project that we will is use to demonstrate this migration is made only by two classes, a loan application public class LoanApplication {    private String id;    private Applicant applicant;    private int amount;    private int deposit;    private boolean approved = false;    public LoanApplication(String id, Applicant applicant, int amount, int deposit) {        this.id = id;        this.applicant = applicant;        this.amount = amount;        this.deposit = deposit;    } } and the applicant who requested it public class Applicant {    private int age;    public Applicant(String name, int age) {        this.name = name;        this.age = age;    } } The rules set is made of business decisions to approve or reject an application plus one last rule collecting all the approved applications into a list. The rules set is made of business decisions to approve or reject an application plus one last rule collecting all the approved applications into a list. global Integer maxAmount; global java.util.List approvedApplications; rule LargeDepositApprove when    $l: LoanApplication( applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ) then    modify($l) { setApproved(true) }; // loan is approved end rule LargeDepositReject when    $l: LoanApplication( applicant.age &gt;= 20, deposit &gt;= 1000, amount &gt; maxAmount ) then    modify($l) { setApproved(false) }; // loan is rejected end // ... more loans approval/rejections business rules ... rule CollectApprovedApplication when    $l: LoanApplication( approved ) then    approvedApplications.add($l); // collect all approved loan applications end STEP 1: EXPOSING RULES EVALUATION WITH A REST ENDPOINT THROUGH QUARKUS The first goal that we want to achieve is providing a REST endpoint for this service using Quarkus. The easiest way to do this is creating a new module depending on the one containing the rules plus a few basic Quarkus libraries providing the REST support. &lt;dependencies&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;    &lt;artifactId&gt;quarkus-resteasy&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;io.quarkus&lt;/groupId&gt;    &lt;artifactId&gt;quarkus-resteasy-jackson&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;    &lt;groupId&gt;org.example&lt;/groupId&gt;    &lt;artifactId&gt;drools-project&lt;/artifactId&gt;    &lt;version&gt;1.0-SNAPSHOT&lt;/version&gt;  &lt;/dependency&gt; &lt;dependencies&gt; With this setup it’s easy to create a REST endpoint as it follows. @Path("/find-approved") public class FindApprovedLoansEndpoint {    private static final KieContainer kContainer = KieServices.Factory.get().newKieClasspathContainer();    @POST()    @Produces(MediaType.APPLICATION_JSON)    @Consumes(MediaType.APPLICATION_JSON)    public List&lt;LoanApplication&gt; executeQuery(LoanAppDto loanAppDto) {        KieSession session = kContainer.newKieSession();        List&lt;LoanApplication&gt; approvedApplications = new ArrayList&lt;&gt;();        session.setGlobal("approvedApplications", approvedApplications);        session.setGlobal("maxAmount", loanAppDto.getMaxAmount());        loanAppDto.getLoanApplications().forEach(session::insert);        session.fireAllRules();        session.dispose();        return approvedApplications;    } } Here a KieContainer containing the rules taken from the other module in the classpath is created and put into a static field. In this way it will be possible to reuse the same KieContainer for all subsequent invocations of this endpoint without having to recompile the rules. When the endpoint is invoked it creates a new KieSession from the container, populates it with the objects coming from a DTO resulting from the unmarshalling of the JSON request. public class LoanAppDto {    private int maxAmount;    private List&lt;LoanApplication&gt; loanApplications;    public int getMaxAmount() {        return maxAmount;    }    public void setMaxAmount(int maxAmount) {        this.maxAmount = maxAmount;    }    public List&lt;LoanApplication&gt; getLoanApplications() {        return loanApplications;    }    public void setLoanApplications(List&lt;LoanApplication&gt; loanApplications) {        this.loanApplications = loanApplications;    } } When we call fireAllRules() the session is fired and all our business logic is evaluated against the provided input data. Then the last rule collects all the approved applications into a global list and this list is returned as the result of the computation. After having started Quarkus you can already put this at work invoking the REST endpoint with a JSON request containing the loan applications to be checked and the value for the maxAmount to be used in the rules, like in the the following example curl -X POST -H 'Accept: application/json' -H 'Content-Type: application/json' -d '{"maxAmount":5000,"loanApplications":[ {"id":"ABC10001","amount":2000,"deposit":1000,"applicant":{"age":45,"name":"John"}}, {"id":"ABC10002","amount":5000,"deposit":100,"applicant":{"age":25,"name":"Paul"}}, {"id":"ABC10015","amount":1000,"deposit":100,"applicant":{"age":12,"name":"George"}} ]}' http://localhost:8080/find-approved and you will be returned with a list of the approved applications. [{"id":"ABC10001","applicant":{"name":"John","age":45},"amount":2000,"deposit":1000,"approved":true}] This straightforward approach has the major drawback of not allowing to use some of the most interesting features of Quarkus like the hot reload and the possibility of creating a native image of the project. Those features, as we will see in the next step, are indeed provided by the Kogito extension to Quarkus that in essence makes Quarkus aware of the existence of the drl files,  implementing their hot reload in a similar way to what Quarkus provides out-of-the-box for the java sources. The integration demonstrated up to this point between Drools and Quakus has to be considered no more than an introduction to the next migration step. Since Kogito supports the use of Drools API, and given the advantages it provides in terms of fully functional out-of-the-box Quarkus integration, we strongly suggest to not stop the development of your REST service at this point.  STEP 2: FROM DROOLS TO KOGITO WITHOUT CHANGING (ALMOST) ANYTHING USING THE DROOLS LEGACY API As anticipated, Kogito can provide those missing features, so let’s try to migrate our project to Kogito with the minimal amount of effort. To do this we can use the Quarkus extension for Kogito in conjunction with the kogito-legacy-api allowing us to use the same API of Drools 7. This approach also makes it possible to consolidate the former two modules into a single one. &lt;dependencies&gt;  &lt;dependency&gt;   &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt;   &lt;artifactId&gt;kogito-quarkus-rules&lt;/artifactId&gt;  &lt;/dependency&gt;  &lt;dependency&gt;   &lt;groupId&gt;org.kie.kogito&lt;/groupId&gt;   &lt;artifactId&gt;kogito-legacy-api&lt;/artifactId&gt;  &lt;/dependency&gt; &lt;/dependencies&gt; In this way no changes at all are required to  the DRL file containing the rules while the former REST endpoint implementation can be rewritten as follows. @Path("/find-approved") public class FindApprovedLoansEndpoint {    @Inject    KieRuntimeBuilder kieRuntimeBuilder;    @POST()    @Produces(MediaType.APPLICATION_JSON)    @Consumes(MediaType.APPLICATION_JSON)    public List&lt;LoanApplication&gt; executeQuery(LoanAppDto loanAppDto) {        KieSession session = kieRuntimeBuilder.newKieSession();        List&lt;LoanApplication&gt; approvedApplications = new ArrayList&lt;&gt;();        session.setGlobal("approvedApplications", approvedApplications);        session.setGlobal("maxAmount", loanAppDto.getMaxAmount());        loanAppDto.getLoanApplications().forEach(session::insert);        session.fireAllRules();        session.dispose();        return approvedApplications;    } } Here the only difference with the former implementation is that the KieSession instead of being created from the KieContainer is created from an automatically injected KieRuntimeBuilder.  The KieRuntimeBuilder is the interface provided by the kogito-legacy-api module that replace the KieContainer and from which it is now possible to create the KieBases and KieSessions exactly as you did with the KieContainer itself. An implementation of the KieRuntimeBuilder interface is automatically generated at compile time by Kogito and injected into the class implementing the REST endpoint. With this change it is possible both to launch quarkus in dev mode thus leveraging its hot reload to make on-the-fly changes also to the rules files that are immediately applied to the running application and to create a native image of your rule based application.   STEP 3: EMBRACING RULE UNITS AND AUTOMATIC REST ENDPOINT GENERATION A rule unit is a new concept introduced in Kogito encapsulating both a set of rules and the facts against which those rules will be matched. It comes with a second abstraction called data source, defining the sources through which the facts are inserted, acting in practice as typed entry-points. There are two types of data sources: * DataStream: an append-only data source * subscribers only receive new (and possibly past) messages * cannot update/remove * stream may also be hot/cold in “reactive streams” terminology * DataStore: data source for modifiable data * subscribers may act upon the data store, by acting upon the fact handle In essence a rule unit is made of 2 strictly related parts: the definition of the fact to be evaluated and the set of rules evaluating them. The first part is implemented with a POJO, that for our loan applications could be something like the following: package org.kie.kogito.queries; import org.kie.kogito.rules.DataSource; import org.kie.kogito.rules.DataStore; import org.kie.kogito.rules.RuleUnitData; public class LoanUnit implements RuleUnitData {    private int maxAmount;    private DataStore&lt;LoanApplication&gt; loanApplications;    public LoanUnit() {        this(DataSource.createStore(), 0);    }    public LoanUnit(DataStore&lt;LoanApplication&gt; loanApplications, int maxAmount) {        this.loanApplications = loanApplications;        this.maxAmount = maxAmount;    }    public DataStore&lt;LoanApplication&gt; getLoanApplications() { return loanApplications; }    public void setLoanApplications(DataStore&lt;LoanApplication&gt; loanApplications) {        this.loanApplications = loanApplications;    }    public int getMaxAmount() { return maxAmount; }    public void setMaxAmount(int maxAmount) { this.maxAmount = maxAmount; } } Here instead of using the LoanAppDto that we introduced to marshall/unmarshall the JSON requests we are binding directly the class representing the rule unit. The two relevant differences are that it implements the org.kie.kogito.rules.RuleUnitData interface and uses a DataStore instead of a plain List to contain the loan applications to be approved. The first is just a marker interface to notify the engine that this class is part of a rule unit definition. The use of a DataStore is necessary to let the rule engine to react to changes of processed fact, this allows the rule engine to react accordingly to the changes by firing new rules and triggering other rules. In the example, the consequences of the rules modify the approved property of the loan applications. Conversely the maxAmount value can be considered a configuration parameter of the rule unit and left as it is: it will automatically be processed during the rules evaluation with the same semantic of a global, and automatically set from the value passed by the JSON request as in the first example, so you will still be allowed to use it in your rules. The second part of the rule unit is the drl file containing the rules belonging to this unit. package org.kie.kogito.queries; unit LoanUnit; // no need to using globals, all variables and facts are stored in the rule unit  rule LargeDepositApprove when    $l: /loanApplications[ applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ] // oopath style then    modify($l) { setApproved(true) }; end rule LargeDepositReject when    $l: /loanApplications[ applicant.age &gt;= 20, deposit &gt;= 1000, amount &gt; maxAmount ] then    modify($l) { setApproved(false) }; end // ... more loans approval/rejections business rules ... // approved loan applications are now retrieved through a query query FindApproved    $l: /loanApplications[ approved ] end This rules file must declare the same package and a unit with the same name of the java class implementing the RuleUnitData interface in order to state that they belong to the same rule unit. This file has also been rewritten using the new OOPath notation: as anticipated, here the data source acts as a typed entry-point and the oopath expression has its name as root while the constraints are in square brackets, like in the following example. $l: /loanApplications[ applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ] Alternatively you can still use the old DRL syntax, specifying the name of the data source as an entry-point, with the drawback that in this case you need to specify again the type of the matched object, even if the engine can infer it from the type of the datasource, as it follows.  $l: LoanApplication( applicant.age &gt;= 20, deposit &gt;= 1000, amount &lt;= maxAmount ) from entry-point loanApplications Finally note that the last rule collecting all the approved loan applications into a global List has been replaced by a query simply retrieving them. One of the advantages in using a rule unit is that it clearly defines the context of computation, in other terms the facts to be passed in input to the rule evaluation. Similarly the query defines what is the output expected by this evaluation.      This clear definition of the computation boundaries allows Kogito to also automatically generate a class executing the query and returning its results public class LoanUnitQueryFindApproved implements org.kie.kogito.rules.RuleUnitQuery&lt;List&lt;org.kie.kogito.queries.LoanApplication&gt;&gt; {    private final RuleUnitInstance&lt;org.kie.kogito.queries.LoanUnit&gt; instance;    public LoanUnitQueryFindApproved(RuleUnitInstance&lt;org.kie.kogito.queries.LoanUnit&gt; instance) {        this.instance = instance;    }    @Override    public List&lt;org.kie.kogito.queries.LoanApplication&gt; execute() {        return instance.executeQuery("FindApproved").stream().map(this::toResult).collect(toList());    }    private org.kie.kogito.queries.LoanApplication toResult(Map&lt;String, Object&gt; tuple) {        return (org.kie.kogito.queries.LoanApplication) tuple.get("$l");    } } together with a REST endpoint taking the rule unit as input, passing it to the former query executor and returning its as output. @Path("/find-approved") public class LoanUnitQueryFindApprovedEndpoint {    @javax.inject.Inject    RuleUnit&lt;org.kie.kogito.queries.LoanUnit&gt; ruleUnit;    public LoanUnitQueryFindApprovedEndpoint() {    }    public LoanUnitQueryFindApprovedEndpoint(RuleUnit&lt;org.kie.kogito.queries.LoanUnit&gt; ruleUnit) {        this.ruleUnit = ruleUnit;    }    @POST()    @Produces(MediaType.APPLICATION_JSON)    @Consumes(MediaType.APPLICATION_JSON)    public List&lt;org.kie.kogito.queries.LoanApplication&gt; executeQuery(org.kie.kogito.queries.LoanUnit unit) {        RuleUnitInstance&lt;org.kie.kogito.queries.LoanUnit&gt; instance = ruleUnit.createInstance(unit);        return instance.executeQuery(LoanUnitQueryFindApproved.class);    } } You can have as many query as you want and for each of them it will be generated a different REST endpoint with the same name of the query transformed from camel case (like FindApproved) to dash separated (like find-approved). The post appeared first on .&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/GDkJ8DmV0lM" height="1" width="1" alt=""/&gt;</content><dc:creator>Mario Fusco</dc:creator><feedburner:origLink>https://blog.kie.org/2021/09/bringing-drools-rules-into-the-cloud-with-kogito-a-step-by-step-path.html</feedburner:origLink></entry><entry><title>Faster web deployment with Python serverless functions</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/Wob7oaCf6hA/faster-web-deployment-python-serverless-functions" /><author><name>Don Schenck</name></author><id>07578a68-80d3-4ada-9dc6-9d404b0f335b</id><updated>2021-09-02T07:00:00Z</updated><published>2021-09-02T07:00:00Z</published><summary type="html">&lt;p&gt;Functions as a Service (FaaS) and &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;serverless architecture&lt;/a&gt; promise quick, lightweight deployments for web applications and other standalone functions. But until recently, creating FaaS in &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt; has been a "sort of" process consisting of multiple steps. You weren't really creating a function so much as an application that could scale back to zero pods after a few minutes, then scale up again when called.&lt;/p&gt; &lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;Red Hat OpenShift Serverless Functions&lt;/a&gt; is a newer feature that changes all of that. As a developer, you can use it to deploy functions in a snap. You can scaffold functions that handle HTTP requests or &lt;a href="https://cloudevents.io/"&gt;CloudEvents&lt;/a&gt; with one command.&lt;/p&gt; &lt;p&gt;This article gets you started with creating and deploying serverless functions with OpenShift Serverless Functions. We'll use &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; to develop our serverless function, but it's just one of many languages you could choose from.&lt;/p&gt; &lt;h2&gt;Creating and deploying serverless functions with Knative&lt;/h2&gt; &lt;p&gt;OpenShift Serverless Functions uses the open source &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;Knative&lt;/a&gt; framework, which offers powerful management tools through its &lt;code&gt;kn&lt;/code&gt; command-line interface (CLI). Prior to OpenShift Serverless Functions, creating a function in OpenShift required writing an application from scratch, using Knative to manage the application, and creating the deployment, service, and route to support the application. While creating a serverless function that way was not terribly complicated, OpenShift Serverless Functions makes life much easier.&lt;/p&gt; &lt;p&gt;With OpenShift Serverless Functions, developers no longer have to worry about creating the deployment, service, and route. It's all one thing: The function. You can't get more &lt;em&gt;serverless&lt;/em&gt; than that.&lt;/p&gt; &lt;p&gt;Deploying a function with OpenShift Serverless Functions requires three Knative commands:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;kn func create kn func build kn func deploy &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;There's more to the process, but those three commands get to the heart of it. We'll explore more about deployment shortly. First, we need to set up our environment to support OpenShift Serverless Functions.&lt;/p&gt; &lt;h2&gt;Step 1: Set up your serverless development environment&lt;/h2&gt; &lt;p&gt;I was able to complete all of my examples for this article using &lt;a href="https://developers.redhat.com/products/codeready-containers/overview"&gt;Red Hat CodeReady Containers&lt;/a&gt;. CodeReady Containers requires at least 9GB of RAM. I also had to set the number of CPUs to five in order to get both HTTP-driven and event-driven functions to run at the same time. Note that I issued this command &lt;em&gt;before&lt;/em&gt; starting CodeReady Containers:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;crc config set cpus 5&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;When I used a more enterprise-like, cloud-based OpenShift cluster—as you might find in a typical OpenShift installation—CPU and memory usage was not a concern.&lt;/p&gt; &lt;h3&gt;The OpenShift Serverless Operator&lt;/h3&gt; &lt;p&gt;Before you can start deploying functions to an OpenShift cluster, you must install the OpenShift Serverless Operator. From the OpenShift console, locate the operator's card, click on it, and then use the default values to install it. When the installation is finished, the dashboard will let you know. When you see the "Installed operator — ready for use" message shown in Figure 1, click the &lt;strong&gt;View Operator&lt;/strong&gt; button.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_installed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_installed.png?itok=wQQGBTVK" width="607" height="227" alt="After you successfully install the Red Hat OpenShift Serverless Operator, the OpenShift dashboard shows the message "Installed operator — ready for use."" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: The OpenShift dashboard showing the Red Hat OpenShift Serverless Operator is ready for use. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You will see your OpenShift Serverless Operator in all its glory, as shown in Figure 2.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_openshift.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_openshift.png?itok=3XPyIrR4" width="977" height="474" alt="The Red Hat OpenShift Serverless Operator offers three APIs: Knative Serving, Knative Eventing, and Knative Kafka." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: The OpenShift dashboard showing the APIs offered by the Red Hat OpenShift Serverless Operator. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;The Knative Serving API&lt;/h3&gt; &lt;p&gt;With the operator in place, your next step is to prepare the Knative Serving API. Change the project you're working in to &lt;strong&gt;knative-serving&lt;/strong&gt;, as shown in Figure 3. That's where the API must be located.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_serving.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_serving.png?itok=bOjbeCBo" width="360" height="596" alt="Change the current project to Knative Serving in OpenShift Serverless Functions." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Choosing the knative-serving project in OpenShift Serverless Functions. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Once that's done, click on the &lt;strong&gt;Knative Serving&lt;/strong&gt; link under the Provided APIs, click &lt;strong&gt;Create Knative Serving&lt;/strong&gt;, and use the default values to create the API.&lt;/p&gt; &lt;p&gt;When all of the statuses read True, as shown in Figure 4, you are ready to start using OpenShift Serverless Functions for HTTP-based functions.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_true.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_true.png?itok=omHA_hE6" width="708" height="280" alt="Knative Serving is ready when all statuses are True." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Status of Knative Serving. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;The Knative Eventing API&lt;/h3&gt; &lt;p&gt;You need to perform the steps in this section if you want to use CloudEvents to fire your functions. In this case, we'll use Knative Eventing with CloudEvents. The steps are similar if you want to use Knative Serving instead.&lt;/p&gt; &lt;p&gt;Change the working project to &lt;strong&gt;knative-eventing&lt;/strong&gt; and make sure the OpenShift Serverless Function Operator page is displayed.&lt;/p&gt; &lt;p&gt;Click on the &lt;strong&gt;Knative Eventing&lt;/strong&gt; link under the Provided APIs, then click &lt;strong&gt;Create Knative Eventing&lt;/strong&gt;. Use the default values to create the API.&lt;/p&gt; &lt;p&gt;When all of the statuses at the bottom of the page read &lt;strong&gt;True&lt;/strong&gt;, you are ready to start using OpenShift Serverless Functions for CloudEvent-based functions.&lt;/p&gt; &lt;p&gt;That's it: We're finished with all of the installation tasks. Our cluster will now support both HTTP-based and CloudEvent-based serverless functions.&lt;/p&gt; &lt;h2&gt;Step 2: Create an HTTP serverless function in Python&lt;/h2&gt; &lt;p&gt;You can create an HTTP serverless function using Knative's &lt;code&gt;kn&lt;/code&gt; CLI, and the function will be fully functional. You do have to edit the code, of course, to do what you want.&lt;/p&gt; &lt;p&gt;The steps required to create a basic function are shown in Figure 5. In a terminal window, create a directory whose name will become the name of the function. Then, move into that directory and create the function using the &lt;code&gt;kn func create&lt;/code&gt; command. The default runtime is &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;, which we will not be using. Instead, we'll use the following command to create a serverless function written in Python:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kn func create -l python&lt;/code&gt;&lt;/pre&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_basic.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_basic.png?itok=U01f42x2" width="646" height="299" alt="The steps to create a basic application culminate in a "kn func create" command." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: The steps to create a serverless function using Python. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: Why did I choose Python? It's popular, I have a Python &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservice&lt;/a&gt; that I'm going to convert to a function (in my next article), and Red Hat Developer already has a &lt;a href="https://developers.redhat.com/articles/2021/07/01/nodejs-serverless-functions-red-hat-openshift-part-1-logging"&gt; series of articles about creating OpenShift Serverless Functions with Node.js&lt;/a&gt;. So, Python it is.&lt;/p&gt; &lt;h3&gt;About the kn func create command&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;kn func create&lt;/code&gt; command uses the name of the current directory to create the source code for a function. Any supporting files, such as dependencies, will also be created. You simply start with this template and edit the function to suit your needs.&lt;/p&gt; &lt;p&gt;If no language is specified, Node.js will be the default. Several languages are supported, and the list seems to be growing at a decent pace. For now, you can specify any of the following:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Go&lt;/li&gt; &lt;li&gt;Node.js&lt;/li&gt; &lt;li&gt;Python&lt;/li&gt; &lt;li&gt;Quarkus&lt;/li&gt; &lt;li&gt;Rust&lt;/li&gt; &lt;li&gt;Spring Boot&lt;/li&gt; &lt;li&gt;TypeScript&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Enter this command to see the list of currently supported languages:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ kn func create --help&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Figure 6 shows where the list of languages appears in the output.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_languages.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_languages.png?itok=jo27UbOi" width="1087" height="709" alt="Output of the "kn func create --help" command shows which languages it supports." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Languages supported by the "kn func create" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Creating the Knative function&lt;/h3&gt; &lt;p&gt;So what just happened in our &lt;code&gt;kn&lt;/code&gt; command? Figure 7 shows a listing in the directory after we run &lt;code&gt;kn func create -l python&lt;/code&gt;.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_create.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_create.png?itok=w8RukE92" width="612" height="240" alt="The "kn func create" command adds files for a basic application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: The content of project directory after running the "kn func create" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Let's look inside the &lt;code&gt;func.py&lt;/code&gt; file to see what was created and how it will be used:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-python"&gt;from parliament import Context def main(context: Context): """ Function template The context parameter contains the Flask request object and any CloudEvent received with the request. """ return { "message": "Howdy!" }, 200&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;As shown, this skeleton Python function returns "Howdy!" Remove the comments and you can see that it only takes three lines of code to make a working function. As a default function, the skeleton function is not very useful. My next article will update it to read from a database, so stay tuned.&lt;/p&gt; &lt;p&gt;Note that we've also created the &lt;code&gt;func.yaml&lt;/code&gt; file. If you view the contents, you will notice that it is incomplete. For example, the &lt;code&gt;image&lt;/code&gt; field is empty. If you wish, you can edit this file to create the image name and tag. The default will be the function name and the &lt;code&gt;:latest&lt;/code&gt; tag:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;name: python-faas-example namespace: "" runtime: python image: "" imageDigest: "" builder: quay.io/boson/faas-python-builder:v0.8.3 builderMap: default: quay.io/boson/faas-python-builder:v0.8.3 volumes: [] envs: [] annotations: {} options: {}&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Step 3: Build the Python serverless function&lt;/h2&gt; &lt;p&gt;We can build our default HTTP-based function by running the &lt;code&gt;kn func build&lt;/code&gt; command. But because the image name was not specified in the &lt;code&gt;func.yaml&lt;/code&gt; file, this command will prompt us for an image registry. It will use the registry name, the function name, and the tag &lt;code&gt;:latest&lt;/code&gt; to create the image name—if you haven't already supplied one by editing the YAML file. For my own functions, I use my registry: &lt;code&gt;docker.io/donschenck&lt;/code&gt;.&lt;/p&gt; &lt;p&gt;Figure 8 shows the &lt;code&gt;kn func build&lt;/code&gt; command and the resulting &lt;code&gt;func.yaml&lt;/code&gt; file. Notice that the fully-qualified image name has been generated by the command. I'm using PowerShell in Windows, but a Bash shell terminal in macOS or Linux works just as well. The operating system you choose doesn't affect the results: You can build functions anywhere.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_build.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_build.png?itok=UiWYu5dl" width="721" height="353" alt="The "kn func build" command creates a minimal YAML configuration file." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: YAML configuration created by the "kn func build" command. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;If you view your local image registry, shown in Figure 9, you will see that the image now exists. (I have no idea why "41 years ago" appears.)&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_images.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_images.png?itok=L-OTemY7" width="1255" height="102" alt="A "docker images" command shows the image you created." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 9: A "docker images" command showing the existence of an image. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Testing the function&lt;/h3&gt; &lt;p&gt;You can use the &lt;code&gt;kn func run&lt;/code&gt; command to run the function locally and test it. In this case, the function will run on port 8080.&lt;/p&gt; &lt;h2&gt;Step 4: Deploy the Python serverless function&lt;/h2&gt; &lt;p&gt;With the function built into an image on your local machine, it's time to deploy it to a cluster. Before you can do that, you need to sign into two systems: The image registry you're using (mine is &lt;code&gt;docker.io/donschenck&lt;/code&gt;) and the cluster where you wish to deploy the function. You also need to make sure you're in the correct project. Figure 10 shows an example of what to do.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_logins.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_logins.png?itok=yy7mz71E" width="1179" height="506" alt="To prepare for deployment to OpenShift, you must log in to the image registry and your OpenShift cluster, and create a new project." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 10: Summary of logins and creation of a project in OpenShift. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;When you run &lt;code&gt;kn func deploy&lt;/code&gt;, the command builds the image, pushes the image to the image registry you specified, and then deploys that image from the registry into the OpenShift project to which your current context is set.&lt;/p&gt; &lt;p&gt;In this case, the &lt;code&gt;docker.io/donschenck/python-faas-example:latest&lt;/code&gt; image is deployed to the &lt;code&gt;faas-example&lt;/code&gt; project in my cluster, as shown in Figure 11.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_deploy.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_deploy.png?itok=wVpOkk3z" width="1179" height="84" alt="The "kn func deploy" command can get an application into your cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 11: Output from the "kn func deploy" command in a cluster. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p class="Indent1"&gt;&lt;strong&gt;Note&lt;/strong&gt;: You can edit the &lt;code&gt;func.yaml&lt;/code&gt; file and change the image tag if you wish. I changed my tag from &lt;code&gt;:latest&lt;/code&gt; to &lt;code&gt;:v1&lt;/code&gt; and it works just fine.&lt;/p&gt; &lt;p&gt;Figure 12 shows the developer topology view in the OpenShift dashboard, displaying the deployed function.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_dashboard.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_dashboard.png?itok=WfB0MVxa" width="504" height="404" alt="The OpenShift dashboard shows that your function is deployed and has running instances." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 12: The OpenShift dashboard showing the deployed function. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;You can prove that the function in the cluster is working simply by clicking on the &lt;strong&gt;Open URL&lt;/strong&gt; icon.&lt;/p&gt; &lt;h3&gt;Watch the HTTP function scale to zero&lt;/h3&gt; &lt;p&gt;Wait a bit and you'll see the dark blue circle in the function turn white (see Figure 13). This means the function is still available, but it has scaled down to zero pods.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_scaled.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_scaled.png?itok=IjKvh6A5" width="509" height="400" alt="When the blue circle around a function disappears in the OpenShift dashboard, the function has scaled down to zero pods." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 13: The function after it has scaled down to zero pods. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;If you access the function now—by clicking on the &lt;strong&gt;Open URL&lt;/strong&gt; icon, or refreshing the browser where you previously opened it—you'll see a slight delay before getting the result. This delay happens only when the function is scaling from zero to one. Refresh yet again and you'll see a speedy response. The function is now up and running.&lt;/p&gt; &lt;h3&gt;Update the function&lt;/h3&gt; &lt;p&gt;Updating the function requires the following steps, which are shown in Figure 14:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Edit the &lt;code&gt;func.py&lt;/code&gt; source file.&lt;/li&gt; &lt;li&gt;Run the &lt;code&gt;kn func deploy&lt;/code&gt; command.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;That's all you need to do. The &lt;code&gt;kn func deploy&lt;/code&gt; command &lt;em&gt;automagically&lt;/em&gt; rebuilds the image before pushing it to your image registry and deploying it to your cluster.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_updated.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_updated.png?itok=9guPcOWj" width="1173" height="296" alt="Updating a function requires editing the source code and redeploying it with the "kn func deploy" command." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 14: Steps needed to update a function. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Troubleshooting kn func deploy&lt;/h2&gt; &lt;p&gt;Before closing, let's look at some common error messages related to &lt;code&gt;kn func deploy&lt;/code&gt; and how to recover from them.&lt;/p&gt; &lt;h3&gt;incorrect username or password&lt;/h3&gt; &lt;p&gt;This message, shown in Figure 15, occurred to me once when I ran &lt;code&gt;kn func deploy&lt;/code&gt; while I was not logged into my docker.io registry.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_incorrect.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_incorrect.png?itok=mKtPZy5O" width="476" height="25" alt="An invalid password or username when logging in prevents deployment of the functon to the image registry." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 15: An "incorrect username or password" error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;The lesson is that you must be logged into the image register in order to successfully run the command, because it has to push the image to the repository. The &lt;code&gt;kn&lt;/code&gt; command was nice enough to prompt me for username and password, but I made a mistake entering them. Of course, my function was not deployed as a result. When I supplied the correct name and password, the command worked.&lt;/p&gt; &lt;h3&gt;knative deployer failed to get the Knative Service&lt;/h3&gt; &lt;p&gt;This happened to me when I ran &lt;code&gt;kn func deploy&lt;/code&gt; while I was not logged into my OpenShift cluster, as shown in Figure 16. The deployment failed.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content-full-width"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_failed.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_full_width_1440px_w/public/ofs_failed.png?itok=ngLoq_Tf" width="1016" height="64" alt="If you are not logged in to your project in your OpenShift cluster, the "kn func deploy" command cannot get access to the cluster." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 16: A "knative deployer failed to get the Knative Service" error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Although the &lt;code&gt;kn&lt;/code&gt; command can gracefully log in to the image repository, as shown in the previous section, it cannot automatically connect to a cluster. Make sure to log in to the cluster and the correct project, then rerun the &lt;code&gt;kn&lt;/code&gt; command.&lt;/p&gt; &lt;h3&gt;timeout&lt;/h3&gt; &lt;p&gt;I got this error when I ran &lt;code&gt;kn func deploy&lt;/code&gt; while using Red Hat's quay.io as my image registry, as shown in Figure 17.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/ofs_timeout.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/ofs_timeout.png?itok=VjsK8Byi" width="600" height="12" alt="A timeout error may appear when you don't explicitly make images Public in Red Hat's quay.io registry." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 17: A "timeout" error message. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;By default, images added to quay.io registry are marked Private, so your OpenShift cluster cannot pull the image. Simply change the repository visibility in quay.io to Public. OpenShift will continue to attempt to pull the image, and once it is publicly available, the deployment will succeed.&lt;/p&gt; &lt;h2&gt;What else can I do with Python serverless functions?&lt;/h2&gt; &lt;p&gt;Look for the next article in this series, where we'll build a Python-based serverless function that responds to a CloudEvent instead of an HTTP request. Also visit the &lt;a href="https://developers.redhat.com/topics/serverless-architecture"&gt;OpenShift Serverless&lt;/a&gt; homepage to learn more about creating, scaling, and managing serverless functions on &lt;a href="https://developers.redhat.com/products/openshift/"&gt;Red Hat OpenShift&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Serverless functions in Java and Node.js&lt;/h2&gt; &lt;p&gt;Are you interested in writing serverless functions in &lt;a href="https://developers.redhat.com/topics/enterprise-java"&gt;Java&lt;/a&gt; or &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt;? Start with &lt;a href="https://developers.redhat.com/blog/2021/01/04/create-your-first-serverless-function-with-red-hat-openshift-serverless-functions"&gt;this overview of OpenShift serverless functions&lt;/a&gt;, then get a quick tutorial introduction to &lt;a href="https://developers.redhat.com/blog/2021/01/29/write-a-quarkus-function-in-two-steps-on-red-hat-openshift-serverless"&gt;writing a Quarkus function in two steps&lt;/a&gt; or &lt;a href="https://developers.redhat.com/articles/2021/07/01/nodejs-serverless-functions-red-hat-openshift-part-1-logging"&gt;developing Node.js serverless functions&lt;/a&gt; on Red Hat OpenShift.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/02/faster-web-deployment-python-serverless-functions" title="Faster web deployment with Python serverless functions"&gt;Faster web deployment with Python serverless functions&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/Wob7oaCf6hA" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-09-02T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/02/faster-web-deployment-python-serverless-functions</feedburner:origLink></entry><entry><title>The outbox pattern with Apache Kafka and Debezium</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/VFcQCM_rYaU/outbox-pattern-apache-kafka-and-debezium" /><author><name>Don Schenck</name></author><id>fde9b237-3f15-4ece-a630-b4f9c5c97aee</id><updated>2021-09-01T12:00:00Z</updated><published>2021-09-01T12:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/topics/microservices"&gt;Microservices&lt;/a&gt; need access to shared data. Microservices also need to be loosely coupled. Just how are developers supposed to reconcile these diametrically opposed ideas?&lt;/p&gt; &lt;p&gt;Enter &lt;a href="https://kafka.apache.org/"&gt;Apache Kafka&lt;/a&gt;, &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt;, and &lt;a href="https://microservices.io/patterns/data/transactional-outbox.html"&gt;the outbox pattern&lt;/a&gt;. By combining messaging and change data capture technologies with good programming practices, you can meet both microservices demands with ease.&lt;/p&gt; &lt;p&gt;&lt;a href="https://www.youtube.com/watch?v=wEhr-mnPOeQ"&gt;This video explores the outbox pattern and demonstrates it in action&lt;/a&gt;, using &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://debezium.io/"&gt;Debezium&lt;/a&gt;. In just minutes, you’ll see how easy it is to replicate data to your microservices while keeping them loosely coupled. OpenShift Streams for Apache Kafka is a managed cloud solution with a free-to-try introductory option.&lt;/p&gt; &lt;p&gt;When you’re finished, you can use a free account on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; and &lt;a href="https://developers.redhat.com/developer-sandbox/activities/connecting-to-your-managed-kafka-instance"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; to start your own education and experimentation.&lt;/p&gt; &lt;p&gt;You can explore &lt;a href="https://developers.redhat.com/articles/2021/08/11/how-maximize-data-storage-microservices-and-kubernetes-part-1-introduction"&gt;other data solutions within Red Hat OpenShift&lt;/a&gt; as well. The choices are many, so begin your journey today.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/01/outbox-pattern-apache-kafka-and-debezium" title="The outbox pattern with Apache Kafka and Debezium"&gt;The outbox pattern with Apache Kafka and Debezium&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/VFcQCM_rYaU" height="1" width="1" alt=""/&gt;</summary><dc:creator>Don Schenck</dc:creator><dc:date>2021-09-01T12:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/01/outbox-pattern-apache-kafka-and-debezium</feedburner:origLink></entry><entry><title>Red Hat CodeReady Containers 1.31.2 makes the leap</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/50_u54cQr0w/red-hat-codeready-containers-1312-makes-leap" /><author><name>CodeReady Containers Team</name></author><id>a24b60ae-7ffc-470d-8a00-111abeca5c91</id><updated>2021-09-01T07:00:00Z</updated><published>2021-09-01T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/blog/2019/09/05/red-hat-openshift-4-on-your-laptop-introducing-red-hat-codeready-containers"&gt;Red Hat CodeReady Containers&lt;/a&gt; supports local development and testing on a &lt;a href="https://developers.redhat.com/openshift"&gt;Red Hat OpenShift&lt;/a&gt; cluster. We recently released CodeReady Containers 1.31.2, which is the first version based on the major &lt;a href="https://cloud.redhat.com/blog/red-hat-openshift-4.8-is-now-generally-available"&gt;OpenShift 4.8&lt;/a&gt; release. The CodeReady Containers team doesn't publicly report our advances on a regular basis, so this article is a good opportunity to learn about the biggest changes to CodeReady Containers during the past several months.&lt;/p&gt; &lt;h2&gt;Upgrade to OpenShift 4.8&lt;/h2&gt; &lt;p&gt;With this release, we updated CodeReady Containers to use the 4.8 release of OpenShift. This release offers support for single-node clusters as a developer preview. We also enabled OpenShift’s Machine Config Operator, so users can now follow OpenShift documentation for registry and proxy configuration, or for any other changes that use the Machine Config Operator to modify the cluster. In previous releases, these changes required steps specific to CodeReady Containers. The following video shows how to enable the Machine Config Operator (MCO).&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;h2&gt;Installers&lt;/h2&gt; &lt;p&gt;On macOS and Windows 10, we are now shipping native installers (&lt;code&gt;.pkg&lt;/code&gt; and &lt;code&gt;.msi&lt;/code&gt; files, respectively). The native installers provide an easier installation procedure, with more integrated requirement checks. The following video shows how installation on macOS can be done in less than 30 seconds.&lt;/p&gt; &lt;div class="video-embed-field-provider-youtube video-embed-field-responsive-video"&gt; &lt;/div&gt; &lt;p&gt;The installers are signed so that they can be properly validated by the operating system before installation, which is particularly important in light of recent supply-chain attacks. Installers are the only supported way to install and use CodeReady Containers on macOS and Windows.&lt;/p&gt; &lt;h2&gt;System trays&lt;/h2&gt; &lt;p&gt;The new installers on macOS and Windows come with a system tray icon. This icon allows direct interactions with CodeReady Containers, such as to start and stop your cluster. You no longer need to fall back to a shell prompt to manage your OpenShift instance.&lt;/p&gt; &lt;h2&gt;New networking stack&lt;/h2&gt; &lt;p&gt;One of the biggest pain points for CodeReady Containers over the years has been networking, particularly in corporate environments. Configuring the host system DNS to redirect the &lt;code&gt;crc.testing&lt;/code&gt; domain to the CodeReady Containers virtual machine often required superuser privileges. Additionally, corporate VPNs or firewalls would sometimes get in the way and prevent this setup from working, resulting in cluster connectivity issues for end users.&lt;/p&gt; &lt;p&gt;A few releases ago, we started moving to a userland networking stack based on &lt;a href="https://github.com/containers/gvisor-tap-vsock"&gt;gvisor&lt;/a&gt;. All networking communication by the virtual machine now goes through a CodeReady Containers daemon running on the host. Together with improved usage of the &lt;code&gt;/etc/hosts&lt;/code&gt; file for DNS resolution, this change makes the networking setup less reliant on modifications to the host configuration. It also avoids some of the aforementioned issues with corporate networks. This new networking stack is now the default on Windows and macOS.&lt;/p&gt; &lt;h2&gt;Disk expansion&lt;/h2&gt; &lt;p&gt;CodeReady Containers instances use a 31GB disk image by default. This is not enough for some users who want to deploy heavy workloads.&lt;/p&gt; &lt;p&gt;It’s now possible, when running &lt;code&gt;crc start&lt;/code&gt;, to use the &lt;code&gt;--disk-size&lt;/code&gt; or &lt;code&gt;-d&lt;/code&gt; command-line option to dynamically resize the disk to the desired size. The disk size can only be expanded, not reduced. The following example uses a 40GB disk for the CodeReady Containers instance:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ crc start --disk-size 40&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On macOS and Windows, you can also use the system tray icon to easily change the disk size.&lt;/p&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;This article has summarized the most notable changes the CodeReady Containers team made during the past few months, but there were also plenty more minor improvements and bug fixes. We strongly encourage you to &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_codeready_containers/1.31"&gt;try CodeReady Containers 1.31.2&lt;/a&gt; and report any problems you find.&lt;/p&gt; &lt;p&gt;We’ll keep polishing and improving CodeReady Containers in the months to come. Our roadmap includes Podman integration and improved integration with remote CodeReady Containers instances, so stay tuned!&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/09/01/red-hat-codeready-containers-1312-makes-leap" title="Red Hat CodeReady Containers 1.31.2 makes the leap"&gt;Red Hat CodeReady Containers 1.31.2 makes the leap&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/50_u54cQr0w" height="1" width="1" alt=""/&gt;</summary><dc:creator>CodeReady Containers Team</dc:creator><dc:date>2021-09-01T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/09/01/red-hat-codeready-containers-1312-makes-leap</feedburner:origLink></entry><entry><title type="html">An Extension for Long Running Activities</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/w-jurrmJCHY/" /><author><name>Michael Musgrove</name></author><id>https://quarkus.io/blog/using-lra/</id><updated>2021-09-01T00:00:00Z</updated><content type="html">Introduction The Quarkus LRA extension is useful for building JAX-RS services that wish to definitively agree when an interaction has finished, with either a successful outcome or an unsuccessful one. In the successful case, all participants can clean up in the knowledge that all other services will do so as...&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/w-jurrmJCHY" height="1" width="1" alt=""/&gt;</content><dc:creator>Michael Musgrove</dc:creator><feedburner:origLink>https://quarkus.io/blog/using-lra/</feedburner:origLink></entry><entry><title>Why should I choose Quarkus over Spring for my microservices?</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/ma5T-Pjb6MM/why-should-i-choose-quarkus-over-spring-my-microservices" /><author><name>Eric Deandrea</name></author><id>f6484334-5e02-4fbb-bdd4-365927273e1c</id><updated>2021-08-31T14:00:00Z</updated><published>2021-08-31T14:00:00Z</published><summary type="html">&lt;p&gt;As interest grows in &lt;a href="https://developers.redhat.com/topics/microservices"&gt;microservices&lt;/a&gt; and &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt;, &lt;a href="https://developers.redhat.com/topics/enterprise-java/"&gt;Java&lt;/a&gt; developers have struggled to make applications smaller and faster to meet today’s demands and requirements. In the modern computing environment, applications must respond to requests quickly and efficiently, be suitable for running in volatile environments such as virtual machines or containers, and support rapid development. Because of this, Java, and popular Java runtimes, are sometimes considered inferior to runtimes in other languages such as &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; and &lt;a href="https://golang.org/"&gt;Go&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;The Java language and platform have been very successful over the years, preserving Java as the predominant language in current use. &lt;a href="https://www.grandviewresearch.com/industry-analysis/application-server-market"&gt;Analysts have estimated the global application server market size&lt;/a&gt; at $15.84 billion in 2020, with expectations of growing at a rate of 13.2% from 2021 to 2028. Additionally, tens of millions of Java developers worldwide work for organizations that run their businesses using Java. Faced with today’s challenges, these organizations need to adapt and adopt new ways of building and deploying applications. Forgoing Java for other application stacks isn’t a choice for many organizations. It would involve re-training their development staff and re-implementing processes to release and monitor applications in production.&lt;/p&gt; &lt;h2&gt;Java is still relevant&lt;/h2&gt; &lt;p&gt;With additions to Java and Java frameworks over the past few years, Java can proudly retain its role as the primary language for enterprise applications. &lt;a href="https://developers.redhat.com/products/quarkus/getting-started"&gt;Quarkus&lt;/a&gt;, an open source project introduced by Red Hat, is one such framework that has taken the Java community by storm. Quarkus combines developer productivity and joy with the speed and performance of Go.&lt;/p&gt; &lt;p&gt;Quarkus integrates with other modern Java frameworks and libraries that many developers are already familiar with and most likely using. Among the specifications and technologies underlying and integrated with Quarkus are Eclipse MicroProfile, Eclipse Vert.x, Contexts and Dependency Injection (CDI), Jakarta RESTful Web Services (JAX-RS), the Java Persistence API (JPA), the Java Transaction API (JTA), Apache Camel, and Hibernate, just to name a few.&lt;/p&gt; &lt;p&gt;Quarkus and &lt;a href="https://developers.redhat.com/topics/spring-boot/"&gt;Spring&lt;/a&gt; address many of the same types of applications, but because it was born in today’s day and age, Quarkus has the advantage of starting with a clean slate. Quarkus can focus on innovation in modern areas of development pertaining to scalable, cloud-hosted applications because it doesn’t have to retrofit new patterns and principles into an existing codebase that has evolved over time.&lt;/p&gt; &lt;h2&gt;But I already know Spring ...&lt;/h2&gt; &lt;p&gt;&lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; introduces Quarkus to Java developers with a special eye to helping those familiar with Spring’s concepts, constructs, and conventions learn Quarkus quickly. Spring developers should immediately recognize and be able to apply patterns they are already familiar with, and in many instances, using the same underlying technologies. Using Kotlin in your Spring applications? Great—you can continue using Kotlin with Quarkus.&lt;/p&gt; &lt;p&gt;Chapters are devoted to getting started, RESTful applications, persistence, &lt;a href="https://developers.redhat.com/topics/event-driven/"&gt;event-driven&lt;/a&gt; services, and cloud environments such as containers and &lt;a href="https://developers.redhat.com/topics/kubernetes/"&gt;Kubernetes&lt;/a&gt;. Each chapter offers like-for-like examples and emphasizes testing patterns and practices with such applications, while also differentiating Quarkus from Spring.&lt;/p&gt; &lt;p&gt;Additionally, Quarkus provides a set of &lt;a href="https://quarkus.io/guides/spring-di#more-spring-guides"&gt;extensions for various Spring APIs&lt;/a&gt;. These extensions help simplify the process of learning Quarkus or migrating existing Spring applications to Quarkus, capitalizing on a developer’s Spring knowledge to accelerate the learning curve to adopt Quarkus. In some cases, an &lt;a href="https://developers.redhat.com/blog/2021/02/09/spring-boot-on-quarkus-magic-or-madness"&gt;existing Spring application may even be able to run in Quarkus&lt;/a&gt; without any code changes.&lt;/p&gt; &lt;h2&gt;How can Quarkus help me?&lt;/h2&gt; &lt;p&gt;Quarkus has many features and capabilities that can help both developers and operations teams.&lt;/p&gt; &lt;h3&gt;Enhancing developer productivity&lt;/h3&gt; &lt;p&gt;Since its inception in early 2019, Quarkus has focused on more than just delivering features. Developer productivity and joy have been critical goals. With every new feature, Quarkus carefully considers the developer experience and how to improve it.&lt;/p&gt; &lt;p&gt;The development process is faster and more pleasant with Quarkus's live coding feature. Quarkus can automatically detect changes made to Java and other resource and configuration files, then transparently re-compile and re-deploy the changes. Usually, within a second, you can view your application’s output or compiler error messages. This feature can also be used with Quarkus applications running in a remote environment. The remote capability is useful where rapid development or prototyping is needed but provisioning services in a local environment isn’t feasible or possible.&lt;/p&gt; &lt;p&gt;Quarkus takes this concept a step further with its &lt;a href="https://youtu.be/0JiE-bRt-GU"&gt;continuous testing&lt;/a&gt; feature to facilitate test-driven development. As changes are made to the application source code, Quarkus can automatically rerun affected tests in the background, giving developers instant feedback about the code they are writing or modifying.&lt;/p&gt; &lt;p&gt;Need a database for your application? Kafka broker? Redis server? AMQP broker? OpenID Connect authentication server? API/Schema registry? Quarkus &lt;a href="https://quarkus.io/guides/datasource#dev-services-configuration-free-databases"&gt;Dev Services for databases&lt;/a&gt; (see the &lt;a href="https://youtu.be/szza3DZlKzA"&gt;video demo&lt;/a&gt;), &lt;a href="https://quarkus.io/guides/kafka-dev-services"&gt;Dev Services for Kafka&lt;/a&gt; (see the &lt;a href="https://youtu.be/z2ZceqVQ20E"&gt;video demo&lt;/a&gt;), &lt;a href="https://quarkus.io/guides/redis-dev-services"&gt;Dev Services for Redis&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/amqp-dev-services"&gt;Dev Services for AMQP&lt;/a&gt;, &lt;a href="https://quarkus.io/guides/security-openid-connect-dev-services"&gt;Dev Services for OpenID Connect&lt;/a&gt; (see the &lt;a href="https://youtu.be/coG5ZbLgjJs"&gt;video demo&lt;/a&gt;), and &lt;a href="https://quarkus.io/guides/apicurio-registry-dev-services"&gt;Dev Services for Apicurio Registry&lt;/a&gt; have you covered. Dev Services makes development faster by providing needed infrastructure automatically, eliminating all the required provisioning and configuration hassle. New Dev Services are added with each new release.&lt;/p&gt; &lt;p&gt;Following the philosophy of simplicity and enhancing developer productivity, &lt;a href="https://quarkus.io/guides/building-native-image"&gt;building an application into a native image&lt;/a&gt; is extremely simple. All the heavy-lifting and integration to consume GraalVM is done for you by the Quarkus build tools. Developers or &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; systems simply need to run a build, just like any other Java build, to produce a native executable. Tests can even be run against the built artifact.&lt;/p&gt; &lt;h3&gt;Kubernetes native&lt;/h3&gt; &lt;p&gt;From the beginning, Quarkus was designed around Kubernetes-native philosophies, optimizing for low memory usage and fast startup times. As much processing as possible is done at build time. Classes used only at application startup are invoked at build time and not loaded into the runtime JVM, reducing the size, and ultimately the memory footprint, of the application running on the JVM.&lt;/p&gt; &lt;p&gt;This design accounted for native compilation from the onset, enabling Quarkus to be "natively native." Similar native capabilities in Spring are still considered experimental or beta, and in some instances, not even available. Coupled with a runtime platform like Kubernetes, more Quarkus applications can be deployed within a given set of resources than other Java or Spring applications.&lt;/p&gt; &lt;h3&gt;To be or not to be reactive?&lt;/h3&gt; &lt;p&gt;With Spring, a developer needs to decide up front, before writing a line of code, which architecture to follow for an application. This choice determines the entire set of libraries that a developer uses in a Spring application. Quarkus does not have such limitations because it was born in the reactive era. Quarkus, at its core, is based on a fully reactive and non-blocking architecture powered by &lt;a href="https://vertx.io"&gt;Eclipse Vert.x&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;Quarkus integrates deeply with Vert.x, allowing developers to utilize both blocking (imperative) and non-blocking (reactive) libraries and APIs. In most cases, developers can use both blocking and reactive APIs within the same classes. &lt;a href="https://quarkus.io/blog/resteasy-reactive-smart-dispatch/"&gt;Quarkus ensures&lt;/a&gt; that the blocking APIs will block appropriately while the reactive APIs remain non-blocking.&lt;/p&gt; &lt;h2&gt;Ready to give it a try?&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://red.ht/dev-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;, which offers a free and ready-made environment for trying out containerized applications, is &lt;a href="https://developers.redhat.com/articles/2021/05/31/learn-quarkus-faster-quick-starts-developer-sandbox-red-hat-openshift"&gt;set up to support Quarkus&lt;/a&gt;. Whether you run on-premises, on &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Red Hat OpenShift&lt;/a&gt;, or in another cloud setting, the many open source capabilities of Quarkus are available.&lt;/p&gt; &lt;p&gt;&lt;a href="https://red.ht/quarkus-spring-devs"&gt;&lt;em&gt;Quarkus for Spring Developers&lt;/em&gt;&lt;/a&gt; will help get you started and guide you through your journey.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices" title="Why should I choose Quarkus over Spring for my microservices?"&gt;Why should I choose Quarkus over Spring for my microservices?&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/ma5T-Pjb6MM" height="1" width="1" alt=""/&gt;</summary><dc:creator>Eric Deandrea</dc:creator><dc:date>2021-08-31T14:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/31/why-should-i-choose-quarkus-over-spring-my-microservices</feedburner:origLink></entry><entry><title>Game telemetry with Kafka Streams and Quarkus, Part 2</title><link rel="alternate" href="http://feedproxy.google.com/~r/jbossbuzz/~3/qjWAG2-BvTg/game-telemetry-kafka-streams-and-quarkus-part-2" /><author><name>Evan Shortiss</name></author><id>755954fe-2569-44ba-949c-fdd7393a6cc4</id><updated>2021-08-31T07:00:00Z</updated><published>2021-08-31T07:00:00Z</published><summary type="html">&lt;p&gt;The &lt;a href="https://developers.redhat.com/articles/2021/08/24/game-telemetry-kafka-streams-and-quarkus-part-1"&gt;first half of this article&lt;/a&gt; introduced &lt;a href="https://arcade.redhat.com/shipwars"&gt;Shipwars&lt;/a&gt;, a browser-based video game that’s similar to the classic &lt;a href="https://en.wikipedia.org/wiki/Battleship_(game)"&gt;Battleship&lt;/a&gt; tabletop game, but with a server-side AI opponent. We set up a development environment to analyze real-time gaming data and I explained some of the ways you might use game data analysis and telemetry data to improve a product.&lt;/p&gt; &lt;p&gt;In this second half, we'll run the analytics and use the captured data to replay games.&lt;/p&gt; &lt;h2&gt;Using Kafka Streams for game analytics&lt;/h2&gt; &lt;p&gt;We're using the &lt;a href="https://kafka.apache.org/documentation/streams/"&gt;Kafka Streams API&lt;/a&gt; along with &lt;a href="https://developers.redhat.com/products/red-hat-openshift-streams-for-apache-kafka/overview"&gt;Red Hat OpenShift Streams for Apache Kafka&lt;/a&gt; and &lt;a href="https://www.openshift.com/products/application-services"&gt;Red Hat OpenShift Application Services&lt;/a&gt; in the &lt;a href="https://developers.redhat.com/developer-sandbox"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt;. Please see the first half of this article to set up the environment for analyzing data captured during gameplay.&lt;/p&gt; &lt;p&gt;The Shipwars architecture contains several independent Java applications that use the Kafka Streams API. The source code for these applications is in the &lt;a href="https://github.com/evanshortiss/shipwars-streams"&gt;shipwars-streams repository&lt;/a&gt; on GitHub. We'll deploy each of the applications for game data analysis.&lt;/p&gt; &lt;h2&gt;Deploy the Kafka Streams enricher&lt;/h2&gt; &lt;p&gt;The first Kafka Streams application you’ll deploy is an &lt;em&gt;enricher&lt;/em&gt;. It performs a join between two sources of data: Players and Attacks (also known as "shots"). Specifically, this application performs a &lt;a href="https://kafka.apache.org/20/documentation/streams/developer-guide/dsl-api.html#kstream-globalktable-join"&gt;KStream-GlobalKTable join&lt;/a&gt;. The join involves deserializing the JSON data from two Kafka topics into POJOs using &lt;a href="https://kafka.apache.org/10/documentation/streams/developer-guide/datatypes"&gt;Serdes&lt;/a&gt; for serialization and deserialization. Then, we use the &lt;a href="https://kafka.apache.org/documentation/streams/developer-guide/"&gt;Kafka Streams DSL&lt;/a&gt; to join the data sources.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;GlobalKTable&lt;/code&gt; stores key-value pairs. In this application, the key is the player ID and the value is the player data; that is, the username and whether they are a (supposed) human or an AI bot. An entry is added to this table each time a player is created by the game server, because the game server emits an event with the player data to the &lt;code&gt;shipwars-players&lt;/code&gt; topic that the &lt;code&gt;GlobalKTable&lt;/code&gt; is subscribed to.&lt;/p&gt; &lt;p&gt;A &lt;code&gt;KStream&lt;/code&gt; is an abstraction above a stream of events in a specific topic. Kafka Streams applications can map, filter, and even join &lt;code&gt;KStream&lt;/code&gt; instances with one another and instances of &lt;code&gt;GlobalKTable&lt;/code&gt;. The Shipwars application demonstrates this feature.&lt;/p&gt; &lt;p&gt;Every attack made in Shipwars arrives on a &lt;code&gt;KStream&lt;/code&gt; subscribed to the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic. It is then joined with the associated player data in the &lt;code&gt;GlobalKTable&lt;/code&gt; to create a new record. The relevant code is included in Figure 1. Also see the &lt;a href="https://github.com/evanshortiss/shipwars-streams/blob/main/shot-stream-enricher/src/main/java/org/acme/kafka/streams/enricher/streams/TopologyShotMapper.java#L29-L82"&gt;a TopologyShotMapper repository&lt;/a&gt; on GitHub.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_1.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_1.png?itok=bkI7ybai" width="600" height="688" alt="The enricher is a Kafka Streams application that performs a KStream-GlobalKTable join and writes the join result to a new Kafka topic." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 1: Kafka Streams DSL code for the enricher application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Deploy the application&lt;/h3&gt; &lt;p&gt;Take the following steps to deploy the Kafka Streams enricher application:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;Navigate to your project in the Developer Sandbox UI.&lt;/li&gt; &lt;li&gt;Ensure that the &lt;strong&gt;Developer&lt;/strong&gt; view is selected in the side menu.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;+Add&lt;/strong&gt; link and select &lt;strong&gt;Container Image&lt;/strong&gt; from the available options.&lt;/li&gt; &lt;li&gt;Paste &lt;code&gt;quay.io/evanshortiss/shipwars-streams-shot-enricher&lt;/code&gt; into the &lt;strong&gt;Image name field&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Select Quarkus as the &lt;strong&gt;Runtime icon&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Under &lt;strong&gt;General&lt;/strong&gt;, select &lt;strong&gt;Create Application&lt;/strong&gt; and enter &lt;code&gt;shipwars-analysis&lt;/code&gt; as the &lt;strong&gt;Application name&lt;/strong&gt;.&lt;/li&gt; &lt;li&gt;Leave the &lt;strong&gt;Name&lt;/strong&gt; as the default value.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Deployment&lt;/strong&gt; link under the &lt;strong&gt;Advanced options&lt;/strong&gt;. This reveals a new form element to enter environment variables.&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Add from ConfigMap or Secret&lt;/strong&gt; button and create the following variables (also shown in Figure 2): &lt;ol&gt;&lt;li&gt;&lt;code&gt;KAFKA_BOOTSTRAP_SERVERS&lt;/code&gt;: Select the generated &lt;code&gt;shipwars&lt;/code&gt; secret and use the &lt;code&gt;bootstrapServers&lt;/code&gt; key as the value.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KAFKA_CLIENT_ID&lt;/code&gt;: Select the &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt; secret and use the &lt;code&gt;client-id&lt;/code&gt; value.&lt;/li&gt; &lt;li&gt;&lt;code&gt;KAFKA_CLIENT_SECRET&lt;/code&gt;: Select the &lt;code&gt;rh-cloud-services-service-account&lt;/code&gt; secret and use the &lt;code&gt;client-secret&lt;/code&gt; value.&lt;/li&gt; &lt;/ol&gt;&lt;/li&gt; &lt;li&gt;Click the &lt;strong&gt;Create&lt;/strong&gt; button to deploy the application.&lt;/li&gt; &lt;/ol&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_2.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_2.jpg?itok=dSf893Vt" width="600" height="366" alt="The Deployment screen in the OpenShift console can configure the Kafka Streams enricher application." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 2: Configuring the Kafka Streams enricher application. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Play a round of Shipwars&lt;/h3&gt; &lt;p&gt;Now you’ve successfully deployed the first Kafka Streams application. It should appear as a &lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; icon in the OpenShift Topology view. The blue ring around it indicates that the application is in a healthy running state.&lt;/p&gt; &lt;p&gt;Next, open the &lt;code&gt;shipwars-client&lt;/code&gt; NGINX application URL, then open the Kafka Streams &lt;code&gt;shipwars-streams-shot-enricher&lt;/code&gt; application pod logs in another browser window. Play a match of Shipwars and watch the pod logs as you play. The &lt;code&gt;shipwars-streams-shot-enricher&lt;/code&gt; application prints a log each time it receives an attack event, and joins the event information with the associated player record. This joined record is then written to the &lt;code&gt;shipwars-attacks-lite&lt;/code&gt; Kafka topic.&lt;/p&gt; &lt;p&gt;If you’d like to view the data in the new topic, you can do so using a Kafka client such as &lt;a href="https://github.com/edenhill/kafkacat"&gt;kafkacat&lt;/a&gt;, which is shown in Figure 3.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_3.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_3.jpg?itok=_CETulbe" width="600" height="366" alt="The kafkacat CLI can show enriched shots. All the shots displayed in this screenshot are from the same match between an AI player and a human player. " typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 3: Using the kafkacat CLI to view enriched shots between an AI and a human player. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Deploy the Kafka Streams aggregators&lt;/h2&gt; &lt;p&gt;The next two Kafka Streams applications perform aggregations. These aggregations are stateful operations that track the rolling state of a value for a particular key.&lt;/p&gt; &lt;h3&gt;Shot distribution&lt;/h3&gt; &lt;p&gt;The first application, the shot distribution aggregator, aggregates the total number of shots against each cell on the 5x5 grid used by Shipwars. This aggregate breaks down into the following integer values:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;AI hit&lt;/li&gt; &lt;li&gt;AI miss&lt;/li&gt; &lt;li&gt;Human hit&lt;/li&gt; &lt;li&gt;Human miss&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;To create this aggregation, you need the data output by the Kafka Streams enricher you deployed previously. The data in the &lt;code&gt;shipwars-attacks&lt;/code&gt; topic doesn’t specify whether the attacker was a human or AI. The joined data does specify this important piece of information.&lt;/p&gt; &lt;p&gt;There are two other interesting aspects of this application:&lt;/p&gt; &lt;ol&gt;&lt;li&gt;It uses &lt;a href="https://github.com/eclipse/microprofile-reactive-streams-operators]"&gt;MicroProfile Reactive Streams &lt;/a&gt;to expose the enriched data stream via HTTP server-sent events. You’ll see this in action soon.&lt;/li&gt; &lt;li&gt;It stores the aggregated data and exposes it for queries via HTTP REST endpoints.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;You will find the Kafka Streams DSL code for the shot distribution aggregator in the &lt;a href="https://github.com/evanshortiss/shipwars-streams/blob/main/shot-distribution-aggregator/src/main/java/org/acme/kafka/streams/aggregator/streams/TopologyShotAnalysis.java"&gt;TopologyShotAnalysis repository&lt;/a&gt;. Figure 4 shows the relevant code.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_4.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_4.png?itok=plS7ahz2" width="600" height="539" alt="Aggregation using the Kafka Streams DSL. This code consumes the enriched attacks or shots, and creates an aggregation that contains counts of hits and misses. The aggregation is keyed for each deployment of the game server. New deployments of the game server result in new aggregate records." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 4: Aggregation using the Kafka Streams DSL. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;We used the second aggregation at Red Hat Summit 2021. It forms a complete record of all turns for each match, so that you can analyze them later or even replay them as we did in our demonstration.&lt;/p&gt; &lt;h3&gt;Deploy the aggregators&lt;/h3&gt; &lt;p&gt;Both applications can be deployed in the exact same manner as the enricher. Simply use &lt;code&gt;quay.io/evanshortiss/shipwars-streams-shot-distribution&lt;/code&gt; and &lt;code&gt;quay.io/evanshortiss/shipwars-streams-match-aggregates&lt;/code&gt; as the &lt;strong&gt;Image name&lt;/strong&gt; and add each to the &lt;code&gt;shipwars-analysis&lt;/code&gt; application that you created for the enricher application. View the logs once each application has started. Assuming you played a Shipwars match, you should immediately see logs stating that the aggregate shots record has been updated for a specific game ID.&lt;/p&gt; &lt;p&gt;Use the &lt;strong&gt;Open URL&lt;/strong&gt; button on the &lt;code&gt;shipwars-streams-shot-distribution&lt;/code&gt; node in the OpenShift Topology view to access the exposed route for the application. Append &lt;code&gt;/shot-distribution&lt;/code&gt; to the opened URL, in the following format:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;http://shipwars-streams-shot-distribution-$USERNAME-dev.$SUBDOMAIN.openshiftapps.com/shot-distribution&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The endpoint will return the aggregations in JSON format, as shown in Figure 5. The top-level keys are the game generation IDs. Each nested key represents a cell (X, Y coordinate) on the game grid, and contains the resulting hits and misses against that cell, classified by player type.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_5.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_5.jpg?itok=uYqD59WW" width="600" height="366" alt="An aggregation of shots for a given game server generation/deployment is shown in a JSON array, divided into hits and misses for the AI and the human player." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 5: Aggregation of shots for a given game server generation. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Using the aggregated data&lt;/h3&gt; &lt;p&gt;This aggregated data could be used in various ways. The &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt; AI move service could query it and adapt the game difficulty based on the hit rate of players. Alternatively, we could use the data to display a heatmap of shot activity, as you’ll see shortly.&lt;/p&gt; &lt;p&gt;You can view aggregated match records using the same approach. Click the &lt;strong&gt;Open URL&lt;/strong&gt; button on the &lt;code&gt;shipwars-streams-match-aggregate&lt;/code&gt; node in the OpenShift Topology view, and append &lt;code&gt;/replays&lt;/code&gt; to the URL to view match records.&lt;/p&gt; &lt;h2&gt;Analyzing attacks in real-time&lt;/h2&gt; &lt;p&gt;We use SmallRye and MicroProfile Reactive Streams, exposed by the &lt;code&gt;shipwars-streams-shot-distribution&lt;/code&gt; application, to create visualizations of the attacks on a real-time heatmap. The application is custom in this case, but it’s common to ingest the data into other analysis tools and systems using Kafka Connect.&lt;/p&gt; &lt;p&gt;You can test this endpoint by starting a cURL request using the following commands, and then playing the Shipwars game in your browser:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ export AGGREGATOR_ROUTE=$(oc get route shipwars-streams-shot-distribution -o jsonpath='{.spec.host}') $ curl http://$AGGREGATOR_ROUTE/shot-distribution/stream&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Your cURL request should print data similar to the result shown in Figure 6.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_6.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_6.jpg?itok=--uxaAa-" width="600" height="366" alt="A cURL command displays a stream of server-sent events." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 6: Using cURL to display a stream of server-sent events. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Deploy the heatmap application&lt;/h3&gt; &lt;p&gt;Once you’re satisfied that the endpoint is working as expected, you can deploy the heatmap application. This is a web-based application that uses TypeScript, Tailwind CSS, and Parcel Bundler. We're using &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js&lt;/a&gt; to run the build tools, and NGINX serves the resulting HTML, CSS, and JavaScript.&lt;/p&gt; &lt;p&gt;Use the &lt;code&gt;oc new-app&lt;/code&gt; command to build the application on the Developer Sandbox via a &lt;a href="https://github.com/openshift/source-to-image"&gt;source-to-image&lt;/a&gt; process, and deploy the application:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;# The builder/tools image export BUILDER=quay.io/evanshortiss/s2i-nodejs-nginx # Source code to build export SOURCE=https://github.com/evanshortiss/shipwars-visualisations # The public endpoint for the API exposing stream data export ROUTE=$(oc get route shipwars-streams-shot-distribution -o jsonpath='{.spec.host}') oc new-app $BUILDER~$SOURCE \ --name shipwars-visualisations \ --build-env STREAMS_API_URL=http://$ROUTE/ \ -l app.kubernetes.io/part-of=shipwars-analysis \ -l app.openshift.io/runtime=nginx&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The new application shows up in the OpenShift Topology view immediately, but you’ll need to wait for the build to complete before the application becomes usable. Click the &lt;strong&gt;Open URL&lt;/strong&gt; button to view the application UI when the build has finished. A loading spinner is displayed initially, but once you start to play Shipwars in another browser window the heatmap will update in real-time, similar to the display in Figure 7.&lt;/p&gt; &lt;figure role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_7.jpg" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_7.jpg?itok=jueBAeNw" width="600" height="366" alt="A heatmap, updated in real-time, shows how much each square has been attacked in Shipwars. Dark squares have been targeted by more attacks than light squares." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 7: A real-time heatmap shows how much each square has been attacked in Shipwars. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Squares that are targeted frequently will appear darker than squares that are targeted less often. Of course, our game has a low resolution of only 5x5, but in a game with larger maps (such as the first-person shooter games mentioned in the first half of this article) players would tend to spend more time in areas with better weapons and tactical cover. This would result in a useful heatmap for analyzing player behavior.&lt;/p&gt; &lt;h2&gt;Viewing the replays&lt;/h2&gt; &lt;p&gt;The interface we used to view replays at Red Hat Summit 2021 was created by Luke Dary. You can view the aggregated match replays by deploying the UI like so:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ oc new-app quay.io/evanshortiss/shipwars-replay-ui \ -l "app.kubernetes.io/part-of=shipwars-analysis" \ -e REPLAY_SERVER="http://shipwars-streams-match-aggregates:8080" \ --name shipwars-replay $ oc expose svc shipwars-replay&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Once the application is deployed, you can view the replays by opening the application URL in a web browser. The replay UI is shown in Figure 8.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/openshift_kafka_2_8.png" data-featherlight="image"&gt;&lt;img src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/openshift_kafka_2_8.png?itok=wG7F4Nhp" width="600" height="514" alt="Replays of the Shipwars match are aggregated by Kafka Streams." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt; Figure 8: The Shipwars match replays, aggregated by Kafka Streams. &lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you made it this far, well done! You’ve learned how to:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;Use the Red Hat OpenShift Application Services CLI to interact with OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;li&gt;Connect your OpenShift environment to your managed Kafka instances.&lt;/li&gt; &lt;li&gt;Deploy applications into an OpenShift environment using the OpenShift UI and CLI.&lt;/li&gt; &lt;li&gt;Use Kafka Streams to create data processing architectures with OpenShift Streams for Apache Kafka.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;Using an OpenShift cluster and OpenShift Streams for Apache Kafka allows you to focus on building applications instead of infrastructure. Better yet, your applications can run anywhere and still utilize OpenShift Streams for Apache Kafka. The Shipwars application now includes &lt;a href="https://github.com/redhat-gamedev/shipwars-deployment#docker-compose"&gt;instructions for a Docker Compose&lt;/a&gt; deployment that can connect to the managed Kafka service. We suggest giving that a try next.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2021/08/31/game-telemetry-kafka-streams-and-quarkus-part-2" title="Game telemetry with Kafka Streams and Quarkus, Part 2"&gt;Game telemetry with Kafka Streams and Quarkus, Part 2&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;&lt;img src="http://feeds.feedburner.com/~r/jbossbuzz/~4/qjWAG2-BvTg" height="1" width="1" alt=""/&gt;</summary><dc:creator>Evan Shortiss</dc:creator><dc:date>2021-08-31T07:00:00Z</dc:date><feedburner:origLink>https://developers.redhat.com/articles/2021/08/31/game-telemetry-kafka-streams-and-quarkus-part-2</feedburner:origLink></entry></feed>
